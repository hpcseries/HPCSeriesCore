{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Robust Anomaly Detection on Climate Data\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates the difference between **standard z-score** and **robust z-score** anomaly detection.\n",
    "\n",
    "**Key question**: When sensor data contains occasional **spikes** (sensor malfunctions, transmission errors, extreme events), which anomaly detection method is more reliable?\n",
    "\n",
    "We'll use hourly PM2.5 air pollution data from an urban monitoring station that occasionally reports erroneous readings.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: Anomaly Detection Methods\n",
    "\n",
    "### Standard Z-Score Method\n",
    "\n",
    "Flags values that are more than `k` standard deviations from the mean:\n",
    "\n",
    "```\n",
    "z = (x - mean) / std\n",
    "anomaly if |z| > threshold\n",
    "```\n",
    "\n",
    "**Problem**: Mean and std are **heavily influenced by outliers**, so extreme spikes can corrupt the detection.\n",
    "\n",
    "---\n",
    "\n",
    "### Robust Z-Score Method (Modified Z-Score)\n",
    "\n",
    "Uses **median** and **MAD** (Median Absolute Deviation) instead:\n",
    "\n",
    "```\n",
    "MAD = median(|x - median(x)|)\n",
    "robust_z = 0.6745 * (x - median) / MAD\n",
    "anomaly if |robust_z| > threshold\n",
    "```\n",
    "\n",
    "**Advantage**: Median and MAD are **resistant to outliers**, so spikes don't corrupt the detection.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import HPCSeries and load the PM2.5 pollution data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import hpcs\n",
    "\n",
    "# Display library info\n",
    "print(f\"HPCSeries version: {hpcs.__version__}\")\n",
    "print(f\"SIMD ISA: {hpcs.simd_info()['isa']}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load 12 days of hourly PM2.5 readings (288 hours). This dataset contains:\n",
    "- Normal diurnal patterns (low at night, high during day)\n",
    "- **3 sensor error spikes** with extreme values\n",
    "- Realistic air quality variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PM2.5 data\n",
    "df = pd.read_csv('data/climate_pm25_timeseries.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Extract PM2.5 concentration as NumPy array\n",
    "pm25 = df['pm25_ugm3'].values\n",
    "\n",
    "print(f\"Dataset: {len(pm25)} hourly PM2.5 readings\")\n",
    "print(f\"PM2.5 range: {pm25.min():.1f} - {pm25.max():.1f} µg/m³\")\n",
    "print(f\"Mean PM2.5: {pm25.mean():.1f} µg/m³\")\n",
    "print(f\"Median PM2.5: {hpcs.median(pm25):.1f} µg/m³\")\n",
    "print()\n",
    "\n",
    "# Show first few rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Raw Data\n",
    "\n",
    "Plot the raw PM2.5 time-series. Notice the **extreme spikes** that are clearly sensor errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(df['timestamp'], pm25, alpha=0.7, linewidth=1.5, color='steelblue', marker='o', markersize=3)\n",
    "plt.xlabel('Time', fontsize=12)\n",
    "plt.ylabel('PM2.5 (µg/m³)', fontsize=12)\n",
    "plt.title('Hourly PM2.5 Air Pollution — Raw Sensor Data', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Notice the extreme spikes above 150 µg/m³ — these are sensor errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Standard Z-Score Anomaly Detection\n",
    "\n",
    "Detect anomalies using the **standard z-score** method with `hpcs.detect_anomalies()`.\n",
    "\n",
    "Threshold: 3.0 (values more than 3 standard deviations from the mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 3.0\n",
    "\n",
    "# Standard z-score anomaly detection\n",
    "anomalies_standard = hpcs.detect_anomalies(pm25, threshold)\n",
    "\n",
    "num_anomalies = np.sum(anomalies_standard)\n",
    "print(f\"Standard Z-Score Method:\")\n",
    "print(f\"  Detected {num_anomalies} anomalies out of {len(pm25)} readings\")\n",
    "print(f\"  Anomaly rate: {num_anomalies / len(pm25) * 100:.2f}%\")\n",
    "print()\n",
    "\n",
    "# Show detected anomaly values\n",
    "if num_anomalies > 0:\n",
    "    anomaly_values = pm25[anomalies_standard]\n",
    "    print(f\"Detected anomaly values: {anomaly_values}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Robust Z-Score Anomaly Detection\n",
    "\n",
    "Detect anomalies using the **robust z-score** method with `hpcs.detect_anomalies_robust()`.\n",
    "\n",
    "This uses median and MAD instead of mean and std, making it **resistant to outliers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust z-score anomaly detection\n",
    "anomalies_robust = hpcs.detect_anomalies_robust(pm25, threshold)\n",
    "\n",
    "num_anomalies_robust = np.sum(anomalies_robust)\n",
    "print(f\"Robust Z-Score Method:\")\n",
    "print(f\"  Detected {num_anomalies_robust} anomalies out of {len(pm25)} readings\")\n",
    "print(f\"  Anomaly rate: {num_anomalies_robust / len(pm25) * 100:.2f}%\")\n",
    "print()\n",
    "\n",
    "# Show detected anomaly values\n",
    "if num_anomalies_robust > 0:\n",
    "    anomaly_values_robust = pm25[anomalies_robust]\n",
    "    print(f\"Detected anomaly values: {anomaly_values_robust}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Standard vs Robust\n",
    "\n",
    "Let's visualize both detection methods side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(14, 9))\n",
    "\n",
    "# Plot 1: Standard Z-Score\n",
    "axes[0].plot(df['timestamp'], pm25, alpha=0.5, linewidth=1.5, color='steelblue', label='PM2.5 Data')\n",
    "axes[0].scatter(df['timestamp'][anomalies_standard], pm25[anomalies_standard], \n",
    "                color='red', s=80, marker='X', label=f'Anomalies (n={num_anomalies})', zorder=5)\n",
    "axes[0].set_xlabel('Time', fontsize=11)\n",
    "axes[0].set_ylabel('PM2.5 (µg/m³)', fontsize=11)\n",
    "axes[0].set_title('Standard Z-Score Anomaly Detection', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Robust Z-Score\n",
    "axes[1].plot(df['timestamp'], pm25, alpha=0.5, linewidth=1.5, color='steelblue', label='PM2.5 Data')\n",
    "axes[1].scatter(df['timestamp'][anomalies_robust], pm25[anomalies_robust], \n",
    "                color='darkred', s=80, marker='X', label=f'Anomalies (n={num_anomalies_robust})', zorder=5)\n",
    "axes[1].set_xlabel('Time', fontsize=11)\n",
    "axes[1].set_ylabel('PM2.5 (µg/m³)', fontsize=11)\n",
    "axes[1].set_title('Robust Z-Score Anomaly Detection (Median + MAD)', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Which Method is Better?\n",
    "\n",
    "Let's compute the statistics that drive each method to understand the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard statistics (affected by outliers)\n",
    "mean_val = hpcs.mean(pm25)\n",
    "std_val = hpcs.std(pm25)\n",
    "\n",
    "# Robust statistics (resistant to outliers)\n",
    "median_val = hpcs.median(pm25)\n",
    "mad_val = hpcs.mad(pm25)\n",
    "\n",
    "print(\"Statistical Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Standard Method:\")\n",
    "print(f\"  Mean:  {mean_val:.2f} µg/m³\")\n",
    "print(f\"  Std:   {std_val:.2f} µg/m³\")\n",
    "print(f\"  Threshold range: [{mean_val - threshold * std_val:.2f}, {mean_val + threshold * std_val:.2f}]\")\n",
    "print()\n",
    "print(f\"Robust Method:\")\n",
    "print(f\"  Median: {median_val:.2f} µg/m³\")\n",
    "print(f\"  MAD:    {mad_val:.2f} µg/m³\")\n",
    "print(f\"  Threshold range: [{median_val - threshold * mad_val * 1.4826:.2f}, {median_val + threshold * mad_val * 1.4826:.2f}]\")\n",
    "print()\n",
    "print(f\"Key Insight:\")\n",
    "print(f\"  The extreme spikes inflate the mean and std, making the standard method\")\n",
    "print(f\"  less sensitive. The robust method is unaffected by spikes and detects them reliably.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix: Comparing Methods\n",
    "\n",
    "Let's see which anomalies each method detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison\n",
    "both = anomalies_standard & anomalies_robust\n",
    "only_standard = anomalies_standard & ~anomalies_robust\n",
    "only_robust = ~anomalies_standard & anomalies_robust\n",
    "\n",
    "print(\"Detection Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Detected by both methods:     {np.sum(both)}\")\n",
    "print(f\"Detected only by standard:    {np.sum(only_standard)}\")\n",
    "print(f\"Detected only by robust:      {np.sum(only_robust)}\")\n",
    "print()\n",
    "\n",
    "if np.sum(only_robust) > 0:\n",
    "    print(f\"Values detected ONLY by robust method:\")\n",
    "    print(pm25[only_robust])\n",
    "    print(f\"\\nThese are likely the extreme sensor spikes that corrupted the standard method.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Robust Z-Score\n",
    "\n",
    "For time-series data, we can use **rolling robust z-score** to detect anomalies within a moving window.\n",
    "\n",
    "This is useful when the \"normal\" baseline changes over time (e.g., day vs night pollution levels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 24  # 24-hour rolling window\n",
    "\n",
    "# Compute rolling robust z-score\n",
    "rolling_robust_z = hpcs.rolling_robust_zscore(pm25, window)\n",
    "\n",
    "# Flag anomalies (|z| > threshold)\n",
    "rolling_anomalies = np.abs(rolling_robust_z) > threshold\n",
    "\n",
    "print(f\"Rolling Robust Z-Score (window={window} hours):\")\n",
    "print(f\"  Detected {np.sum(rolling_anomalies)} anomalies\")\n",
    "print(f\"  Anomaly rate: {np.sum(rolling_anomalies) / len(pm25) * 100:.2f}%\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Rolling Robust Z-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(14, 9), sharex=True)\n",
    "\n",
    "# Plot 1: Original data with rolling anomalies\n",
    "axes[0].plot(df['timestamp'], pm25, alpha=0.5, linewidth=1.5, color='steelblue', label='PM2.5 Data')\n",
    "axes[0].scatter(df['timestamp'][rolling_anomalies], pm25[rolling_anomalies], \n",
    "                color='darkred', s=80, marker='X', label=f'Anomalies (n={np.sum(rolling_anomalies)})', zorder=5)\n",
    "axes[0].set_ylabel('PM2.5 (µg/m³)', fontsize=11)\n",
    "axes[0].set_title(f'Rolling Robust Anomaly Detection (window={window}h)', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Rolling robust z-score\n",
    "axes[1].plot(df['timestamp'], rolling_robust_z, linewidth=1.5, color='darkgreen', label='Rolling Robust Z-Score')\n",
    "axes[1].axhline(y=threshold, color='red', linestyle='--', linewidth=1.5, label=f'Threshold = ±{threshold}')\n",
    "axes[1].axhline(y=-threshold, color='red', linestyle='--', linewidth=1.5)\n",
    "axes[1].axhline(y=0, color='black', linestyle='-', linewidth=0.5, alpha=0.5)\n",
    "axes[1].fill_between(df['timestamp'], -threshold, threshold, alpha=0.2, color='green', label='Normal Range')\n",
    "axes[1].set_xlabel('Time', fontsize=11)\n",
    "axes[1].set_ylabel('Robust Z-Score', fontsize=11)\n",
    "axes[1].set_title('Rolling Robust Z-Score Values', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmark\n",
    "\n",
    "HPCSeries implements fast anomaly detection using optimized C/C++ kernels.\n",
    "\n",
    "Let's benchmark on a larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create large test array (1 million hourly readings = ~114 years)\n",
    "n = 1_000_000\n",
    "large_data = np.random.randn(n) * 15 + 30  # Simulated PM2.5 data\n",
    "# Add some extreme outliers\n",
    "spike_indices = np.random.choice(n, size=100, replace=False)\n",
    "large_data[spike_indices] = np.random.uniform(200, 500, size=100)\n",
    "\n",
    "print(f\"Benchmark: {n:,} element array\\n\")\n",
    "\n",
    "# Benchmark standard anomaly detection\n",
    "start = time.perf_counter()\n",
    "_ = hpcs.detect_anomalies(large_data, threshold=3.0)\n",
    "elapsed_standard = time.perf_counter() - start\n",
    "print(f\"Standard Anomaly Detection:   {elapsed_standard*1000:.2f} ms  ({n/elapsed_standard/1e6:.1f} M values/sec)\")\n",
    "\n",
    "# Benchmark robust anomaly detection\n",
    "start = time.perf_counter()\n",
    "_ = hpcs.detect_anomalies_robust(large_data, threshold=3.0)\n",
    "elapsed_robust = time.perf_counter() - start\n",
    "print(f\"Robust Anomaly Detection:     {elapsed_robust*1000:.2f} ms  ({n/elapsed_robust/1e6:.1f} M values/sec)\")\n",
    "\n",
    "# Benchmark rolling robust z-score\n",
    "start = time.perf_counter()\n",
    "_ = hpcs.rolling_robust_zscore(large_data, 24)\n",
    "elapsed_rolling = time.perf_counter() - start\n",
    "print(f\"Rolling Robust Z-Score (w=24): {elapsed_rolling*1000:.2f} ms  ({n/elapsed_rolling/1e6:.1f} M values/sec)\")\n",
    "\n",
    "print(f\"\\nRobust method is {elapsed_robust/elapsed_standard:.1f}x slower (due to median computation)\")\n",
    "print(f\"But still processes {n/elapsed_robust/1e3:.0f}k values per second!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What We Learned\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Standard z-score anomaly detection** is fast but **sensitive to outliers**.\n",
    "   - Extreme spikes inflate the mean and std, making detection less reliable.\n",
    "\n",
    "2. **Robust z-score (MAD-based)** is **resistant to outliers**.\n",
    "   - Uses median and MAD which ignore extreme values.\n",
    "   - More reliable for noisy sensor data.\n",
    "\n",
    "3. **Rolling robust z-score** adapts to time-varying baselines.\n",
    "   - Useful for data with diurnal patterns or seasonal trends.\n",
    "\n",
    "4. **HPCSeries provides all three methods** with high performance:\n",
    "   - `hpcs.detect_anomalies()` — Standard z-score\n",
    "   - `hpcs.detect_anomalies_robust()` — Robust z-score (MAD-based)\n",
    "   - `hpcs.rolling_robust_zscore()` — Rolling robust z-score\n",
    "\n",
    "### When to Use Each:\n",
    "\n",
    "| Use Case | Recommended Method |\n",
    "|----------|-------------------|\n",
    "| Clean data with Gaussian distribution | `detect_anomalies()` (faster) |\n",
    "| Sensor data with occasional spikes | `detect_anomalies_robust()` |\n",
    "| Environmental monitoring | `detect_anomalies_robust()` |\n",
    "| Time-series with changing baseline | `rolling_robust_zscore()` |\n",
    "| Financial fraud detection | `detect_anomalies_robust()` |\n",
    "| Real-time stream processing | `rolling_robust_zscore()` |\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "See the next notebook:\n",
    "- **Notebook 3**: Batched rolling analytics for IoT sensors using `hpcs.rolling_mean_batched()`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
