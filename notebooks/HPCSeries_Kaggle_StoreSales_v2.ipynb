{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPCSeries + Kaggle: Store Sales Forecasting\n",
    "\n",
    "**Competition:** [Store Sales - Time Series Forecasting](https://www.kaggle.com/competitions/store-sales-time-series-forecasting)\n",
    "\n",
    "This notebook demonstrates:\n",
    "- **HPCSeries-powered feature engineering** (3-10x faster rolling statistics)\n",
    "- **XGBoost model** optimized for your CPU\n",
    "- **Store Sales prediction** with validation metrics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "\n",
    "Load required libraries and check HPCSeries availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ HPCSeries loaded successfully (v0.7.0)\n",
      "  SIMD ISA:    AVX2\n",
      "  SIMD width:  4 doubles\n",
      "=== HPCS CPU Detection v0.5 ===\n",
      "Logical cores:   8\n",
      "Physical cores:  4\n",
      "Optimal threads: 4\n",
      "\n",
      "L1 cache (KB):   32\n",
      "L2 cache (KB):   512\n",
      "L3 cache (KB):   4096\n",
      "\n",
      "NUMA nodes:      1\n",
      "Cores per node:  4\n",
      "\n",
      "CPU vendor:      AuthenticAMD\n",
      "SIMD width:      256 bits\n",
      "Has SSE2:        T\n",
      "Has AVX:         T\n",
      "Has AVX2:        T\n",
      "Has AVX-512:     F\n",
      "Has NEON:        F\n",
      "Has FMA3:        T\n",
      "================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SIMD] Registered OpenMP SIMD reduction kernels\n",
      "[SIMD] Registered OpenMP SIMD rolling operations\n",
      "[SIMD] Z-score kernels initialized\n",
      "[SIMD Debug] AVX-512=0, AVX2=1, AVX=1, SSE2=1\n",
      "[SIMD] Detected ISA: AVX2 (256-bit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ XGBoost loaded (v3.1.2)\n",
      "âœ“ scikit-learn loaded\n",
      "\n",
      "============================================================\n",
      "Setup complete!\n",
      "============================================================\n",
      "HPCS_AVAILABLE = True\n",
      "XGB_AVAILABLE  = True\n",
      "RANDOM_SEED    = 42\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 1: Environment & Library Setup\n",
    "# ============================================================\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Paths / imports\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Add parent directory to path for local hpcs import\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "# HPCSeries import\n",
    "HPCS_AVAILABLE = False\n",
    "try:\n",
    "    import hpcs\n",
    "\n",
    "    print(f\"âœ“ HPCSeries loaded successfully (v{hpcs.__version__})\")\n",
    "    try:\n",
    "        simd = hpcs.simd_info()\n",
    "        print(f\"  SIMD ISA:    {simd.get('isa', 'unknown')}\")\n",
    "        print(f\"  SIMD width:  {simd.get('width_doubles', '??')} doubles\")\n",
    "    except Exception:\n",
    "        print(\"  (SIMD info not available)\")\n",
    "    HPCS_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ HPCSeries not found: {e}\")\n",
    "    print(\"   Will use NumPy/pandas fallback (slower)\")\n",
    "    HPCS_AVAILABLE = False\n",
    "\n",
    "# XGBoost import\n",
    "XGB_AVAILABLE = False\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "\n",
    "    print(f\"\\nâœ“ XGBoost loaded (v{xgb.__version__})\")\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"\\nâŒ XGBoost not installed. Install with: pip install xgboost\")\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "# scikit-learn for metrics\n",
    "try:\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "    print(\"âœ“ scikit-learn loaded\")\n",
    "except ImportError:\n",
    "    print(\"âŒ scikit-learn not found. Install with: pip install scikit-learn\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Global options (plots / pandas / randomness)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 180)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if XGB_AVAILABLE:\n",
    "    pass\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Small helper utilities\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def df_mem_gb(df: pd.DataFrame, name: str = \"DataFrame\") -> float:\n",
    "    \"\"\"\n",
    "    Print and return memory usage of a DataFrame in GB.\n",
    "    \"\"\"\n",
    "    mem_gb = df.memory_usage(deep=True).sum() / 1024**3\n",
    "    print(f\"ðŸ“¦ {name} memory usage: {mem_gb:.3f} GB (shape={df.shape})\")\n",
    "    return mem_gb\n",
    "\n",
    "\n",
    "def gc_collect(tag: str = \"\"):\n",
    "    \"\"\"\n",
    "    Run garbage collection and optionally log a tag.\n",
    "    \"\"\"\n",
    "    freed = gc.collect()\n",
    "    if tag:\n",
    "        print(f\"ðŸ§¹ GC collected {freed} objects after {tag}\")\n",
    "    return freed\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Setup complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"HPCS_AVAILABLE = {HPCS_AVAILABLE}\")\n",
    "print(f\"XGB_AVAILABLE  = {XGB_AVAILABLE}\")\n",
    "print(f\"RANDOM_SEED    = {RANDOM_SEED}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. Load Meta Data\n",
    "\n",
    "Load the Store Sales competition datasets from local storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata from models/model_metadata.json...\n",
      "âœ… Metadata loaded.\n",
      "  Model version:   hpcs_store_sales_v1_raw_target\n",
      "  Features:        87\n",
      "  Train dates:     2013-01-01 â†’ 2017-07-18\n",
      "  Val dates:       2017-07-19 â†’ 2017-08-15\n",
      "  v1 RMSE:         61.9996\n",
      "  v1 MAE:          21.0376\n",
      "  v1 MAPE:         24.00%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "meta_path = \"models/model_metadata.json\"\n",
    "print(f\"Loading metadata from {meta_path}...\")\n",
    "\n",
    "with open(meta_path, \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "FINAL_FEATURE_COLS = metadata[\"features\"]\n",
    "params_v1          = metadata[\"params\"]\n",
    "metrics_v1         = metadata[\"metrics_v1\"]\n",
    "VAL_HORIZON_DAYS   = metadata[\"val_horizon_days\"]\n",
    "\n",
    "print(\"âœ… Metadata loaded.\")\n",
    "print(f\"  Model version:   {metadata['model_version']}\")\n",
    "print(f\"  Features:        {len(FINAL_FEATURE_COLS)}\")\n",
    "print(f\"  Train dates:     {metadata['train_date_min']} â†’ {metadata['train_date_max']}\")\n",
    "print(f\"  Val dates:       {metadata['val_date_min']} â†’ {metadata['val_date_max']}\")\n",
    "print(f\"  v1 RMSE:         {metrics_v1['rmse']:.4f}\")\n",
    "print(f\"  v1 MAE:          {metrics_v1['mae']:.4f}\")\n",
    "print(f\"  v1 MAPE:         {metrics_v1['mape']:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load Kaggle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Loading core datasets with optimized dtypes...\n",
      "============================================================\n",
      "âœ“ Train loaded: (3000888, 5)\n",
      "ðŸ“¦ train (raw) memory usage: 0.048 GB (shape=(3000888, 5))\n",
      "âœ“ Stores loaded: (54, 5)\n",
      "âœ“ Oil loaded: (1218, 2)\n",
      "âœ“ Holidays loaded: (350, 6)\n",
      "âœ“ Transactions loaded: (83488, 3)\n",
      "ðŸ§¹ GC collected 0 objects after after loading all tables\n",
      "\n",
      "After loading all tables:\n",
      "ðŸ“¦ train (after loading) memory usage: 0.048 GB (shape=(3000888, 5))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.04751371219754219)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 2: Load core Kaggle Store Sales datasets (memory-optimized)\n",
    "# ============================================================\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Adjust this to your actual location\n",
    "DATA_DIR = \"data/kaggle\" \n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Loading core datasets with optimized dtypes...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Core train\n",
    "# ------------------------------------------------------------\n",
    "train_dtypes = {\n",
    "    \"id\": \"int32\",\n",
    "    \"store_nbr\": \"int16\",\n",
    "    \"family\": \"category\",\n",
    "    \"onpromotion\": \"int16\", \n",
    "}\n",
    "\n",
    "train = pd.read_csv(\n",
    "    os.path.join(DATA_DIR, \"train.csv\"),\n",
    "    usecols=[\"date\", \"store_nbr\", \"family\", \"sales\", \"onpromotion\"],\n",
    "    dtype=train_dtypes,\n",
    "    parse_dates=[\"date\"],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "test_dtypes = {\n",
    "    \"id\":        \"int32\",\n",
    "    \"store_nbr\": \"int16\",\n",
    "    \"family\":    \"category\",\n",
    "    \"onpromotion\": \"int16\",\n",
    "}\n",
    "\n",
    "\n",
    "test = pd.read_csv(\n",
    "    os.path.join(DATA_DIR, \"test.csv\"),\n",
    "    usecols=[\"id\", \"date\", \"store_nbr\", \"family\", \"onpromotion\"],\n",
    "    dtype=test_dtypes,\n",
    "    parse_dates=[\"date\"],\n",
    ")\n",
    "\n",
    "# sales as float32 to reduce size\n",
    "train[\"sales\"] = train[\"sales\"].astype(\"float32\")\n",
    "test[\"sales\"] = train[\"sales\"].astype(\"float32\")\n",
    "\n",
    "\n",
    "print(\"âœ“ Train loaded:\", train.shape)\n",
    "df_mem_gb(train, \"train (raw)\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Stores\n",
    "# ------------------------------------------------------------\n",
    "stores = pd.read_csv(\n",
    "    os.path.join(DATA_DIR, \"stores.csv\"),\n",
    "    dtype={\n",
    "        \"store_nbr\": \"int16\",\n",
    "        \"city\": \"category\",\n",
    "        \"state\": \"category\",\n",
    "        \"type\": \"category\",\n",
    "        \"cluster\": \"int8\",\n",
    "    },\n",
    ")\n",
    "print(\"âœ“ Stores loaded:\", stores.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Oil\n",
    "# ------------------------------------------------------------\n",
    "oil = pd.read_csv(\n",
    "    os.path.join(DATA_DIR, \"oil.csv\"),\n",
    "    dtype={\"dcoilwtico\": \"float32\"},\n",
    "    parse_dates=[\"date\"],\n",
    ")\n",
    "print(\"âœ“ Oil loaded:\", oil.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Holidays\n",
    "# ------------------------------------------------------------\n",
    "holidays = pd.read_csv(\n",
    "    os.path.join(DATA_DIR, \"holidays_events.csv\"),\n",
    "    parse_dates=[\"date\"],\n",
    ")\n",
    "\n",
    "for col in [\"type\", \"locale\", \"locale_name\", \"description\"]:\n",
    "    if col in holidays.columns:\n",
    "        holidays[col] = holidays[col].astype(\"category\")\n",
    "\n",
    "print(\"âœ“ Holidays loaded:\", holidays.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Transactions\n",
    "# ------------------------------------------------------------\n",
    "transactions = pd.read_csv(\n",
    "    os.path.join(DATA_DIR, \"transactions.csv\"),\n",
    "    dtype={\n",
    "        \"store_nbr\": \"int16\",\n",
    "        \"transactions\": \"int32\",\n",
    "    },\n",
    "    parse_dates=[\"date\"],\n",
    ")\n",
    "print(\"âœ“ Transactions loaded:\", transactions.shape)\n",
    "\n",
    "gc_collect(\"after loading all tables\")\n",
    "\n",
    "print(\"\\nAfter loading all tables:\")\n",
    "df_mem_gb(train, \"train (after loading)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Data Preprocessing\n",
    "\n",
    "Merge datasets and handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging datasets...\n",
      "============================================================\n",
      "âœ“ Merged train/test with stores\n",
      "Fixing oil prices...\n",
      "âœ“ Fixed oil prices\n",
      "âœ“ Merged train/test with oil\n",
      "Merging transactions into train...\n",
      "âœ“ Merged transactions into train\n",
      "\n",
      "Train shape after merges: (3000888, 11)\n",
      "Test  shape after merges: (28512, 11)\n",
      "\n",
      "Missing values (train):\n",
      "dcoilwtico      857142\n",
      "transactions    245784\n",
      "dtype: int64\n",
      "ðŸ“¦ train (after merges) memory usage: 0.092 GB (shape=(3000888, 11))\n",
      "Approx test  memory (GB): 0.001\n"
     ]
    }
   ],
   "source": [
    "print(\"Merging datasets...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Merge train/test with stores\n",
    "# ------------------------------------------------------------------\n",
    "store_cols = ['store_nbr', 'city', 'state', 'type', 'cluster']\n",
    "stores_small = stores[store_cols].copy()\n",
    "\n",
    "train = train.merge(stores_small, on=\"store_nbr\", how=\"left\")\n",
    "if 'test' in globals():\n",
    "    test = test.merge(stores_small, on=\"store_nbr\", how=\"left\")\n",
    "    print(\"âœ“ Merged train/test with stores\")\n",
    "else:\n",
    "    print(\"âœ“ Merged train with stores (test not loaded yet)\")\n",
    "\n",
    "del stores_small\n",
    "del stores\n",
    "gc.collect()\n",
    "\n",
    "print(\"Fixing oil prices...\")\n",
    "\n",
    "oil['dcoilwtico'] = (\n",
    "    oil['dcoilwtico']\n",
    "    .replace('', np.nan)\n",
    "    .astype('float32')\n",
    "    .ffill()\n",
    "    .bfill()\n",
    ")\n",
    "print(\"âœ“ Fixed oil prices\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Merge oil into train/test\n",
    "# ------------------------------------------------------------------\n",
    "train = train.merge(oil[['date', 'dcoilwtico']], on=\"date\", how=\"left\")\n",
    "if 'test' in globals():\n",
    "    test = test.merge(oil[['date', 'dcoilwtico']], on=\"date\", how=\"left\")\n",
    "    print(\"âœ“ Merged train/test with oil\")\n",
    "else:\n",
    "    print(\"âœ“ Merged train with oil\")\n",
    "\n",
    "del oil\n",
    "gc.collect()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Merge transactions\n",
    "# ------------------------------------------------------------------\n",
    "print(\"Merging transactions into train...\")\n",
    "\n",
    "# Downcast before merging\n",
    "if transactions['transactions'].dtype == 'int64':\n",
    "    transactions['transactions'] = transactions['transactions'].astype('int32')\n",
    "\n",
    "train = train.merge(transactions, on=[\"date\", \"store_nbr\"], how=\"left\")\n",
    "print(\"âœ“ Merged transactions into train\")\n",
    "\n",
    "# Uncomment if your pipeline actually uses transactions for test.\n",
    "# if 'test' in globals():\n",
    "#     test = test.merge(transactions, on=[\"date\", \"store_nbr\"], how=\"left\")\n",
    "#     print(\"âœ“ Merged transactions into test\")\n",
    "\n",
    "del transactions\n",
    "gc.collect()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5) Basic stats\n",
    "# ------------------------------------------------------------------\n",
    "print(f\"\\nTrain shape after merges: {train.shape}\")\n",
    "if 'test' in globals():\n",
    "    print(f\"Test  shape after merges: {test.shape}\")\n",
    "\n",
    "missing_counts = train.isnull().sum()\n",
    "if missing_counts.sum() > 0:\n",
    "    print(\"\\nMissing values (train):\")\n",
    "    print(missing_counts[missing_counts > 0])\n",
    "else:\n",
    "    print(\"\\nâœ“ No missing values in train\")\n",
    "\n",
    "df_mem_gb(train, \"train (after merges)\")\n",
    "if 'test' in globals():\n",
    "    print(\"Approx test  memory (GB):\",\n",
    "          round(test.memory_usage(deep=True).sum() / 1024**3, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Time-Based Features\n",
    "\n",
    "Extract temporal patterns from dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating time features...\n",
      "============================================================\n",
      "âœ“ Added 12 time features\n",
      "  Features: year, month, day, dayofweek, dayofyear...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating time features...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add temporal features with compact dtypes.\"\"\"\n",
    "    date = df['date']\n",
    "\n",
    "    # Calendar components\n",
    "    df['year']        = date.dt.year.astype('int16')    # 2013â€“2017 fits easily\n",
    "    df['month']       = date.dt.month.astype('int8')    # 1â€“12\n",
    "    df['day']         = date.dt.day.astype('int8')      # 1â€“31\n",
    "    df['dayofweek']   = date.dt.dayofweek.astype('int8')  # 0â€“6\n",
    "    df['dayofyear']   = date.dt.dayofyear.astype('int16') # 1â€“366\n",
    "\n",
    "    df['weekofyear']  = date.dt.isocalendar().week.astype('int16')\n",
    "\n",
    "    df['quarter']     = date.dt.quarter.astype('int8')\n",
    "\n",
    "    df['is_weekend']       = (df['dayofweek'] >= 5).astype('int8')\n",
    "    df['is_month_start']   = date.dt.is_month_start.astype('int8')\n",
    "    df['is_month_end']     = date.dt.is_month_end.astype('int8')\n",
    "    df['is_quarter_start'] = date.dt.is_quarter_start.astype('int8')\n",
    "    df['is_quarter_end']   = date.dt.is_quarter_end.astype('int8')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply to train \n",
    "train = add_time_features(train)\n",
    "\n",
    "if 'test' in globals():\n",
    "    test = add_time_features(test)\n",
    "\n",
    "time_features = [\n",
    "    'year', 'month', 'day', 'dayofweek', 'dayofyear', 'weekofyear', 'quarter',\n",
    "    'is_weekend', 'is_month_start', 'is_month_end', 'is_quarter_start', 'is_quarter_end'\n",
    "]\n",
    "\n",
    "print(f\"âœ“ Added {len(time_features)} time features\")\n",
    "print(f\"  Features: {', '.join(time_features[:5])}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. HPCSeries Rolling Features\n",
    "\n",
    "Compute rolling statistics using HPCSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating rolling features with HPCSeries...\n",
      "============================================================\n",
      "Using HPCSeries (SIMD-accelerated)\n",
      "\n",
      "  >>> Window 7 days\n",
      "  Processing 1782 (store, family) groups\n",
      "    Groups processed: 1782/1782 (100.0%)\n",
      "  âœ“ Window 7: features attached\n",
      "\n",
      "  >>> Window 28 days\n",
      "  Processing 1782 (store, family) groups\n",
      "    Groups processed: 1782/1782 (100.0%)\n",
      "  âœ“ Window 28: features attached\n",
      "\n",
      "  >>> Window 56 days\n",
      "  Processing 1782 (store, family) groups\n",
      "    Groups processed: 1782/1782 (100.0%)\n",
      "  âœ“ Window 56: features attached\n",
      "\n",
      "âœ“ Created 12 rolling features in 17.9 seconds\n",
      "  Performance: 167,486 rows/sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"Creating rolling features with HPCSeries...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Ensure deterministic row order and contiguous index for NumPy indexing\n",
    "train = train.sort_values(['store_nbr', 'family', 'date']).reset_index(drop=True)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "windows = [7, 28, 56]\n",
    "\n",
    "if HPCS_AVAILABLE:\n",
    "    print(\"Using HPCSeries (SIMD-accelerated)\")\n",
    "    # You can introspect simd_info() if you like:\n",
    "    # simd = hpcs.simd_info(); print(simd)\n",
    "\n",
    "    def add_rolling_features_hpcs(df: pd.DataFrame, windows) -> tuple[pd.DataFrame, list]:\n",
    "        \"\"\"\n",
    "        Add rolling statistics using HPCSeries SIMD operations.\n",
    "\n",
    "        Strategy:\n",
    "        - No list(groups): we re-create groupby per window (cheap, avoids big temp list)\n",
    "        - For each window:\n",
    "            * Preallocate full-length NumPy arrays (float32) per feature\n",
    "            * Iterate over groups, compute HPC rolling stats for that group\n",
    "            * Assign into arrays by integer position (index after reset_index)\n",
    "        - At the end of each window, attach arrays to df once.\n",
    "        \"\"\"\n",
    "        n = len(df)\n",
    "        rolling_cols: list[str] = []\n",
    "\n",
    "        for window in windows:\n",
    "            print(f\"\\n  >>> Window {window} days\")\n",
    "\n",
    "            col_mean   = f'sales_roll_mean_{window}'\n",
    "            col_median = f'sales_roll_median_{window}'\n",
    "            col_std    = f'sales_roll_std_{window}'\n",
    "            col_mad    = f'sales_roll_mad_{window}'\n",
    "\n",
    "            # Preallocate arrays for this window (float32 to save memory)\n",
    "            arr_mean   = np.full(n, np.nan, dtype=np.float32)\n",
    "            arr_median = np.full(n, np.nan, dtype=np.float32)\n",
    "            arr_std    = np.full(n, np.nan, dtype=np.float32)\n",
    "            arr_mad    = np.full(n, np.nan, dtype=np.float32)\n",
    "\n",
    "            # Fresh groupby per window â€“ avoids storing all subframes in memory\n",
    "            gb = df.groupby(['store_nbr', 'family'], sort=False)\n",
    "            n_groups = gb.ngroups\n",
    "            print(f\"  Processing {n_groups} (store, family) groups\")\n",
    "\n",
    "            for i, (_, group) in enumerate(gb):\n",
    "                idx = group.index.to_numpy()  # 0..n-1 after reset_index\n",
    "                sales = group['sales'].to_numpy(dtype=np.float64)\n",
    "\n",
    "                if len(sales) >= window:\n",
    "                    try:\n",
    "                        # HPCSeries rolling operations (float64 in, float64 out)\n",
    "                        r_mean   = hpcs.rolling_mean(sales, window)\n",
    "                        r_median = hpcs.rolling_median(sales, window)\n",
    "                        r_std    = hpcs.rolling_std(sales, window)\n",
    "                        r_mad    = hpcs.rolling_mad(sales, window)\n",
    "                    except Exception:\n",
    "                        # Fallback: pandas rolling for this group only\n",
    "                        s = group['sales']\n",
    "                        r_mean   = s.rolling(window, min_periods=1).mean().to_numpy()\n",
    "                        r_median = s.rolling(window, min_periods=1).median().to_numpy()\n",
    "                        r_std    = s.rolling(window, min_periods=1).std().to_numpy()\n",
    "                        # You can choose to skip MAD fallback if not needed:\n",
    "                        r_mad    = s.rolling(window, min_periods=1).apply(\n",
    "                            lambda x: np.median(np.abs(x - np.median(x))), raw=False\n",
    "                        ).to_numpy()\n",
    "\n",
    "                    # Assign into full-length arrays, cast down to float32\n",
    "                    arr_mean[idx]   = r_mean.astype(np.float32, copy=False)\n",
    "                    arr_median[idx] = r_median.astype(np.float32, copy=False)\n",
    "                    arr_std[idx]    = r_std.astype(np.float32, copy=False)\n",
    "                    arr_mad[idx]    = r_mad.astype(np.float32, copy=False)\n",
    "\n",
    "                # Progress logging\n",
    "                if (i + 1) % 100 == 0 or (i + 1) == n_groups:\n",
    "                    pct = 100.0 * (i + 1) / n_groups\n",
    "                    print(f\"    Groups processed: {i+1:4d}/{n_groups} ({pct:5.1f}%)\", end='\\r')\n",
    "\n",
    "            # Attach arrays as columns once per window\n",
    "            df[col_mean]   = arr_mean\n",
    "            df[col_median] = arr_median\n",
    "            df[col_std]    = arr_std\n",
    "            df[col_mad]    = arr_mad\n",
    "\n",
    "            rolling_cols.extend([col_mean, col_median, col_std, col_mad])\n",
    "            print(f\"\\n  âœ“ Window {window}: features attached\")\n",
    "\n",
    "        return df, rolling_cols\n",
    "\n",
    "    train, rolling_cols = add_rolling_features_hpcs(train, windows)\n",
    "\n",
    "else:\n",
    "    print(\"Using pandas (slower fallback)\")\n",
    "\n",
    "    rolling_cols = []\n",
    "    for window in windows:\n",
    "        col_mean   = f'sales_roll_mean_{window}'\n",
    "        col_median = f'sales_roll_median_{window}'\n",
    "        col_std    = f'sales_roll_std_{window}'\n",
    "\n",
    "        # Pandas groupby/rolling; cast to float32 to save memory\n",
    "        train[col_mean] = (\n",
    "            train\n",
    "            .groupby(['store_nbr', 'family'])['sales']\n",
    "            .transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "            .astype('float32')\n",
    "        )\n",
    "        train[col_median] = (\n",
    "            train\n",
    "            .groupby(['store_nbr', 'family'])['sales']\n",
    "            .transform(lambda x: x.rolling(window, min_periods=1).median())\n",
    "            .astype('float32')\n",
    "        )\n",
    "        train[col_std] = (\n",
    "            train\n",
    "            .groupby(['store_nbr', 'family'])['sales']\n",
    "            .transform(lambda x: x.rolling(window, min_periods=1).std())\n",
    "            .astype('float32')\n",
    "        )\n",
    "\n",
    "        rolling_cols.extend([col_mean, col_median, col_std])\n",
    "        print(f\"  âœ“ Window {window}\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nâœ“ Created {len(rolling_cols)} rolling features in {elapsed:.1f} seconds\")\n",
    "print(f\"  Performance: {train.shape[0] / elapsed:,.0f} rows/sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.2. Derived Ratio Features\n",
    "\n",
    "Compute relative volatility measures (coefficient of variation, outlier sensitivity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing derived ratio features...\n",
      "============================================================\n",
      "Creating ratio features from rolling statistics...\n",
      "\n",
      "âœ“ Created 9 derived ratio features\n",
      "\n",
      " Derived Features Summary:\n",
      "  MAD/Median (robust CoV):     3 features\n",
      "  STD/MAD (outlier index):     3 features\n",
      "  MAD/Mean (volatility index): 3 features\n",
      "\n",
      " Why these matter:\n",
      "  - Scale-invariant: Compare volatility across stores with different sales volumes\n",
      "  - Outlier-resistant: MAD-based ratios ignore extreme spikes\n",
      "  - Tree-model friendly: Ratios create clear decision boundaries\n",
      "  - Information-rich: Capture distribution shape, not just location/scale\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing derived ratio features...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "eps = np.float32(1e-6) \n",
    "derived_cols = []\n",
    "\n",
    "if len(rolling_cols) > 0:\n",
    "    print(\"Creating ratio features from rolling statistics...\\n\")\n",
    "    \n",
    "    for window in [7, 28, 56]:\n",
    "        col_median = f'sales_roll_median_{window}'\n",
    "        col_mean   = f'sales_roll_mean_{window}'\n",
    "        col_std    = f'sales_roll_std_{window}'\n",
    "        col_mad    = f'sales_roll_mad_{window}'\n",
    "        \n",
    "        train[col_median] = train[col_median].astype('float32')\n",
    "        train[col_mean]   = train[col_mean].astype('float32')\n",
    "        train[col_std]    = train[col_std].astype('float32')\n",
    "        train[col_mad]    = train[col_mad].astype('float32')\n",
    "\n",
    "        # 1. MAD / Median - Robust Coefficient of Variation\n",
    "        col_mad_over_median = f'mad_over_median_{window}'\n",
    "        train[col_mad_over_median] = (\n",
    "            train[col_mad].values.astype('float32') /\n",
    "            (np.abs(train[col_median].values.astype('float32')) + eps)\n",
    "        ).astype('float32')\n",
    "        derived_cols.append(col_mad_over_median)\n",
    "        \n",
    "        # 2. STD / MAD - Outlier Sensitivity Index\n",
    "        col_std_over_mad = f'std_over_mad_{window}'\n",
    "        train[col_std_over_mad] = (\n",
    "            train[col_std].values.astype('float32') /\n",
    "            (train[col_mad].values.astype('float32') + eps)\n",
    "        ).astype('float32')\n",
    "        derived_cols.append(col_std_over_mad)\n",
    "        \n",
    "        # 3. MAD / Mean - Volatility Index (alternative CoV)\n",
    "        col_volatility_idx = f'volatility_index_{window}'\n",
    "        train[col_volatility_idx] = (\n",
    "            train[col_mad].values.astype('float32') /\n",
    "            (np.abs(train[col_mean].values.astype('float32')) + eps)\n",
    "        ).astype('float32')\n",
    "        derived_cols.append(col_volatility_idx)\n",
    "    \n",
    "    print(f\"âœ“ Created {len(derived_cols)} derived ratio features\")\n",
    "    print(f\"\\n Derived Features Summary:\")\n",
    "    print(f\"  MAD/Median (robust CoV):     {len([c for c in derived_cols if 'mad_over_median' in c])} features\")\n",
    "    print(f\"  STD/MAD (outlier index):     {len([c for c in derived_cols if 'std_over_mad' in c])} features\")\n",
    "    print(f\"  MAD/Mean (volatility index): {len([c for c in derived_cols if 'volatility_index' in c])} features\")\n",
    "    \n",
    "    print(f\"\\n Why these matter:\")\n",
    "    print(f\"  - Scale-invariant: Compare volatility across stores with different sales volumes\")\n",
    "    print(f\"  - Outlier-resistant: MAD-based ratios ignore extreme spikes\")\n",
    "    print(f\"  - Tree-model friendly: Ratios create clear decision boundaries\")\n",
    "    print(f\"  - Information-rich: Capture distribution shape, not just location/scale\")\n",
    "    \n",
    "else:\n",
    "    print(\"Skipping derived features (no rolling features found)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.5. HPCSeries Advanced Features\n",
    "\n",
    "Add robust statistics and aggregate features using HPCSeries SIMD operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating advanced HPCSeries features...\n",
      "============================================================\n",
      "Using HPCSeries robust statistics and anomaly detection\n",
      "\n",
      "1. Computing GLOBAL robust z-scores (per-group)...\n",
      "   (Compares each point to entire group history, not just rolling window)\n",
      "   [groups: 1782/1782] (100.0%)\n",
      "   âœ“ Completed global robust z-scores for 1782 groups                    \n",
      "\n",
      "2. Computing per-group aggregate statistics...\n",
      "=== HPCS CPU Detection v0.5 ===\n",
      "Logical cores:   8\n",
      "Physical cores:  4\n",
      "Optimal threads: 4\n",
      "\n",
      "L1 cache (KB):   32\n",
      "L2 cache (KB):   512\n",
      "L3 cache (KB):   4096\n",
      "\n",
      "NUMA nodes:      1\n",
      "Cores per node:  4\n",
      "\n",
      "CPU vendor:      AuthenticAMD\n",
      "SIMD width:      256 bits\n",
      "Has SSE2:        T\n",
      "Has AVX:         T\n",
      "Has AVX2:        T\n",
      "Has AVX-512:     F\n",
      "Has NEON:        F\n",
      "Has FMA3:        T\n",
      "================================\n",
      "   [groups:  800/1782] ( 44.9%)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SIMD Debug] AVX-512=0, AVX2=1, AVX=1, SSE2=1\n",
      "[SIMD] Detected ISA: AVX2 (256-bit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [groups: 1782/1782] (100.0%)\n",
      "   âœ“ Completed aggregate statistics for 1782 groups                    \n",
      "\n",
      "âœ“ Created 9 advanced features in 3.0 seconds\n",
      "  Performance: 992,138 rows/sec\n",
      "\n",
      " Advanced Features Summary:\n",
      "  Global robust z-scores:        4 features (global, clipped, binary flag, anomaly density)\n",
      "  Group aggregates:              5 features (mean, std, median, MAD, std z-score)\n",
      "\n",
      "ðŸ’¡ Key insights from the spec:\n",
      "  - Global vs Rolling: Global compares to entire history, rolling to recent window\n",
      "  - Clipping: Prevents extreme z-scores from dominating tree decisions\n",
      "  - Binary flags: Easier for tree models to split on than continuous z-scores\n",
      "  - Anomaly density: Captures 'volatility clustering' (periods of high activity)\n",
      "  - MAD-based: Robust to outliers (unlike std-based z-scores)\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating advanced HPCSeries features...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "advanced_cols = []\n",
    "start_adv = time.time()\n",
    "\n",
    "if HPCS_AVAILABLE:\n",
    "    print(\"Using HPCSeries robust statistics and anomaly detection\\n\")\n",
    "\n",
    "    # Ensure sorted index is 0..n-1 (we did this in rolling step already,\n",
    "    # but it's cheap to be defensive)\n",
    "    train = train.sort_values(['store_nbr', 'family', 'date']).reset_index(drop=True)\n",
    "\n",
    "    n = len(train)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Preallocate arrays for all advanced features\n",
    "    # ------------------------------------------------------------\n",
    "    rz_global_arr      = np.full(n, np.nan, dtype=np.float32)\n",
    "    rz_clipped_arr     = np.full(n, np.nan, dtype=np.float32)\n",
    "    anom_flag_arr      = np.zeros(n, dtype=np.int8)\n",
    "    anom_count_28_arr  = np.full(n, np.nan, dtype=np.float32)\n",
    "\n",
    "    group_mean_arr     = np.full(n, np.nan, dtype=np.float32)\n",
    "    group_std_arr      = np.full(n, np.nan, dtype=np.float32)\n",
    "    group_median_arr   = np.full(n, np.nan, dtype=np.float32)\n",
    "    group_mad_arr      = np.full(n, np.nan, dtype=np.float32)\n",
    "    anom_score_arr     = np.full(n, np.nan, dtype=np.float32)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 1) GLOBAL robust z-scores (per group)\n",
    "    # ------------------------------------------------------------\n",
    "    print(\"1. Computing GLOBAL robust z-scores (per-group)...\")\n",
    "    print(\"   (Compares each point to entire group history, not just rolling window)\")\n",
    "\n",
    "    gb = train.groupby(['store_nbr', 'family'], sort=False)\n",
    "    n_groups = gb.ngroups\n",
    "\n",
    "    for i, (_, group) in enumerate(gb):\n",
    "        idx = group.index.to_numpy()  # 0..n-1 after reset_index\n",
    "        sales = group['sales'].to_numpy(dtype=np.float64)\n",
    "\n",
    "        if sales.size > 0:\n",
    "            try:\n",
    "                # Global robust z-score: (x - median) / (1.4826 * MAD)\n",
    "                rz_global = hpcs.robust_zscore(sales)  # float64 array\n",
    "\n",
    "                # Clip and anomaly flags\n",
    "                rz_clipped = np.clip(rz_global, -10.0, 10.0)\n",
    "                anom_flags = (np.abs(rz_global) > 3.0).astype(float)\n",
    "\n",
    "                # Rolling anomaly density in 28-day window\n",
    "                if sales.size >= 28:\n",
    "                    anom_count_28 = hpcs.rolling_sum(anom_flags, 28)\n",
    "                else:\n",
    "                    # Not enough history: leave as NaN\n",
    "                    anom_count_28 = np.full_like(anom_flags, np.nan, dtype=np.float64)\n",
    "\n",
    "                # Store into preallocated arrays (cast down to float32 / int8)\n",
    "                rz_global_arr[idx]     = rz_global.astype(np.float32, copy=False)\n",
    "                rz_clipped_arr[idx]    = rz_clipped.astype(np.float32, copy=False)\n",
    "                anom_flag_arr[idx]     = (np.abs(rz_global) > 3.0).astype(np.int8)\n",
    "                anom_count_28_arr[idx] = anom_count_28.astype(np.float32, copy=False)\n",
    "\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if (i + 1) % 100 == 0 or (i + 1) == n_groups:\n",
    "            pct = 100.0 * (i + 1) / n_groups\n",
    "            print(f\"   [groups: {i+1:4d}/{n_groups}] ({pct:5.1f}%)\", end='\\r')\n",
    "\n",
    "    train['robust_zscore_global']  = rz_global_arr\n",
    "    train['robust_zscore_clipped'] = rz_clipped_arr\n",
    "    train['is_anom_robust_3']      = anom_flag_arr\n",
    "    train['anom_count_28']         = anom_count_28_arr\n",
    "\n",
    "    advanced_cols.extend([\n",
    "        'robust_zscore_global',\n",
    "        'robust_zscore_clipped',\n",
    "        'is_anom_robust_3',\n",
    "        'anom_count_28'\n",
    "    ])\n",
    "\n",
    "    print(\"\\n   âœ“ Completed global robust z-scores for \"\n",
    "          f\"{n_groups} groups\" + \" \" * 20)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 2) Per-group aggregate statistics\n",
    "    # ------------------------------------------------------------\n",
    "    print(\"\\n2. Computing per-group aggregate statistics...\")\n",
    "\n",
    "    gb = train.groupby(['store_nbr', 'family'], sort=False)\n",
    "    # n_groups is the same, but we can reuse it\n",
    "\n",
    "    for i, (_, group) in enumerate(gb):\n",
    "        idx = group.index.to_numpy()\n",
    "        sales = group['sales'].to_numpy(dtype=np.float64)\n",
    "\n",
    "        if sales.size > 0:\n",
    "            try:\n",
    "                mean_val   = hpcs.mean(sales)\n",
    "                std_val    = hpcs.std(sales)\n",
    "                median_val = hpcs.median(sales)\n",
    "                mad_val    = hpcs.mad(sales)\n",
    "\n",
    "                # Broadcast group-wide stats to all rows in group\n",
    "                group_mean_arr[idx]   = np.float32(mean_val)\n",
    "                group_std_arr[idx]    = np.float32(std_val)\n",
    "                group_median_arr[idx] = np.float32(median_val)\n",
    "                group_mad_arr[idx]    = np.float32(mad_val)\n",
    "\n",
    "                # Standard anomaly score vs mean/std for comparison\n",
    "                if std_val > 0:\n",
    "                    scores = np.abs(sales - mean_val) / std_val\n",
    "                    anom_score_arr[idx] = scores.astype(np.float32, copy=False)\n",
    "                else:\n",
    "                    anom_score_arr[idx] = 0.0\n",
    "\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if (i + 1) % 100 == 0 or (i + 1) == n_groups:\n",
    "            pct = 100.0 * (i + 1) / n_groups\n",
    "            print(f\"   [groups: {i+1:4d}/{n_groups}] ({pct:5.1f}%)\", end='\\r')\n",
    "\n",
    "    # Attach columns once\n",
    "    train['sales_group_mean']     = group_mean_arr\n",
    "    train['sales_group_std']      = group_std_arr\n",
    "    train['sales_group_median']   = group_median_arr\n",
    "    train['sales_group_mad']      = group_mad_arr\n",
    "    train['sales_anomaly_score']  = anom_score_arr\n",
    "\n",
    "    advanced_cols.extend([\n",
    "        'sales_group_mean',\n",
    "        'sales_group_std',\n",
    "        'sales_group_median',\n",
    "        'sales_group_mad',\n",
    "        'sales_anomaly_score'\n",
    "    ])\n",
    "\n",
    "    print(\"\\n   âœ“ Completed aggregate statistics for \"\n",
    "          f\"{n_groups} groups\" + \" \" * 20)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Timing + summary\n",
    "    # ------------------------------------------------------------\n",
    "    elapsed_adv = time.time() - start_adv\n",
    "    print(f\"\\nâœ“ Created {len(advanced_cols)} advanced features \"\n",
    "          f\"in {elapsed_adv:.1f} seconds\")\n",
    "    print(f\"  Performance: {train.shape[0] / elapsed_adv:,.0f} rows/sec\")\n",
    "\n",
    "    print(f\"\\n Advanced Features Summary:\")\n",
    "    print(f\"  Global robust z-scores:        4 features \"\n",
    "          f\"(global, clipped, binary flag, anomaly density)\")\n",
    "    print(f\"  Group aggregates:              5 features \"\n",
    "          f\"(mean, std, median, MAD, std z-score)\")\n",
    "    print(f\"\\nðŸ’¡ Key insights from the spec:\")\n",
    "    print(f\"  - Global vs Rolling: Global compares to entire history, \"\n",
    "          f\"rolling to recent window\")\n",
    "    print(f\"  - Clipping: Prevents extreme z-scores from dominating \"\n",
    "          f\"tree decisions\")\n",
    "    print(f\"  - Binary flags: Easier for tree models to split on than \"\n",
    "          f\"continuous z-scores\")\n",
    "    print(f\"  - Anomaly density: Captures 'volatility clustering' \"\n",
    "          f\"(periods of high activity)\")\n",
    "    print(f\"  - MAD-based: Robust to outliers (unlike std-based z-scores)\")\n",
    "\n",
    "else:\n",
    "    print(\"HPCSeries not available - skipping advanced features\")\n",
    "    print(\"(These features require robust statistics: MAD, robust_zscore, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.6. Structural Features (Seasonality & Scale)\n",
    "\n",
    "Compute per-store/family structural characteristics that remain constant over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing structural features (seasonality & scale)...\n",
      "============================================================\n",
      "Analyzing seasonality patterns and scale characteristics...\n",
      "\n",
      "1. Computing day-of-week seasonality strength...\n",
      "   (Measures: How much do sales vary by day of week?)\n",
      "   [groups: 1782/1782] (100.0%)\n",
      "   âœ“ Completed seasonality analysis for 1782 groups                    \n",
      "\n",
      "2. Computing Coefficient of Variation (scale-normalized volatility)...\n",
      "   (Formula: CoV = std / |mean| - compares volatility across different scales)\n",
      "   âœ“ Computed CoV from existing group statistics\n",
      "\n",
      "âœ“ Created 2 structural features in 5.3 seconds\n",
      "\n",
      " Structural Features Summary:\n",
      "  Seasonality strength:    1 feature(s)\n",
      "  Scale measures (CoV):    1 feature(s)\n",
      "\n",
      "ðŸ’¡ Why seasonality strength matters:\n",
      "  - Retail data has strong day-of-week patterns (weekends, paydays, etc.)\n",
      "  - Some stores/products have strong patterns, others don't\n",
      "  - Helps model learn: 'Is today's sales unusual for THIS day of week?'\n",
      "  - Example: Weekend surge for entertainment products, weekday for office supplies\n",
      "\n",
      "ðŸ’¡ Why CoV matters:\n",
      "  - Compares volatility across different sales volumes\n",
      "  - High CoV = unstable/unpredictable sales\n",
      "  - Low CoV = stable/predictable sales\n",
      "  - Scale-invariant: 100Â±50 vs 1000Â±500 both have CoV=0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing structural features (seasonality & scale)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "structural_cols = []\n",
    "start_struct = time.time()\n",
    "\n",
    "if HPCS_AVAILABLE and 'dayofweek' in train.columns:\n",
    "    print(\"Analyzing seasonality patterns and scale characteristics...\\n\")\n",
    "\n",
    "    # Ensure deterministic, simple index\n",
    "    train = train.sort_values(['store_nbr', 'family', 'date']).reset_index(drop=True)\n",
    "    n = len(train)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 1) Day-of-week seasonality strength (per group, constant)\n",
    "    # ------------------------------------------------------------\n",
    "    print(\"1. Computing day-of-week seasonality strength...\")\n",
    "    print(\"   (Measures: How much do sales vary by day of week?)\")\n",
    "\n",
    "    # Preallocate array for sf_dow_seasonality_strength\n",
    "    sf_dow_arr = np.full(n, np.nan, dtype=np.float32)\n",
    "\n",
    "    gb = train.groupby(['store_nbr', 'family'], sort=False)\n",
    "    n_groups = gb.ngroups\n",
    "\n",
    "    for i, (_, group) in enumerate(gb):\n",
    "        idx = group.index.to_numpy()\n",
    "\n",
    "        if group.shape[0] > 7: \n",
    "            try:\n",
    "                dow_means = (\n",
    "                    group.groupby('dayofweek')['sales']\n",
    "                         .mean()\n",
    "                         .to_numpy(dtype=np.float64)\n",
    "                )\n",
    "\n",
    "                if dow_means.size > 1:\n",
    "                    # compute std of dow_means using HPCS\n",
    "                    seasonality_strength = hpcs.std(dow_means)\n",
    "                    sf_dow_arr[idx] = np.float32(seasonality_strength)\n",
    "\n",
    "            except Exception:\n",
    "                # leave NaNs for this group\n",
    "                pass\n",
    "\n",
    "        if (i + 1) % 100 == 0 or (i + 1) == n_groups:\n",
    "            pct = 100.0 * (i + 1) / n_groups\n",
    "            print(f\"   [groups: {i+1:4d}/{n_groups}] ({pct:5.1f}%)\", end='\\r')\n",
    "\n",
    "    train['sf_dow_seasonality_strength'] = sf_dow_arr\n",
    "    structural_cols.append('sf_dow_seasonality_strength')\n",
    "    print(\"\\n   âœ“ Completed seasonality analysis for \"\n",
    "          f\"{n_groups} groups\" + \" \" * 20)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 2) Coefficient of Variation (scale-normalized volatility)\n",
    "    # ------------------------------------------------------------\n",
    "    print(\"\\n2. Computing Coefficient of Variation (scale-normalized volatility)...\")\n",
    "    print(\"   (Formula: CoV = std / |mean| - compares volatility across different scales)\")\n",
    "\n",
    "    eps = np.float32(1e-6)\n",
    "\n",
    "    if 'sales_group_std' in train.columns and 'sales_group_mean' in train.columns:\n",
    "        # Ensure base stats are float32\n",
    "        std_vals  = train['sales_group_std'].astype('float32').to_numpy()\n",
    "        mean_vals = train['sales_group_mean'].astype('float32').to_numpy()\n",
    "\n",
    "        cov_vals = std_vals / (np.abs(mean_vals) + eps)\n",
    "        train['sf_cov_sales'] = cov_vals.astype('float32')\n",
    "\n",
    "        structural_cols.append('sf_cov_sales')\n",
    "        print(\"   âœ“ Computed CoV from existing group statistics\")\n",
    "    else:\n",
    "        print(\"   âš ï¸ sales_group_mean/std not found, skipping CoV feature\")\n",
    "\n",
    "    elapsed_struct = time.time() - start_struct\n",
    "    print(f\"\\nâœ“ Created {len(structural_cols)} structural features in \"\n",
    "          f\"{elapsed_struct:.1f} seconds\")\n",
    "\n",
    "    # Summary\n",
    "    print(f\"\\n Structural Features Summary:\")\n",
    "    print(f\"  Seasonality strength:    {len([c for c in structural_cols if 'seasonality' in c])} feature(s)\")\n",
    "    print(f\"  Scale measures (CoV):    {len([c for c in structural_cols if 'cov' in c])} feature(s)\")\n",
    "\n",
    "    print(f\"\\nðŸ’¡ Why seasonality strength matters:\")\n",
    "    print(f\"  - Retail data has strong day-of-week patterns (weekends, paydays, etc.)\")\n",
    "    print(f\"  - Some stores/products have strong patterns, others don't\")\n",
    "    print(f\"  - Helps model learn: 'Is today's sales unusual for THIS day of week?'\")\n",
    "    print(f\"  - Example: Weekend surge for entertainment products, weekday for office supplies\")\n",
    "\n",
    "    print(f\"\\nðŸ’¡ Why CoV matters:\")\n",
    "    print(f\"  - Compares volatility across different sales volumes\")\n",
    "    print(f\"  - High CoV = unstable/unpredictable sales\")\n",
    "    print(f\"  - Low CoV = stable/predictable sales\")\n",
    "    print(f\"  - Scale-invariant: 100Â±50 vs 1000Â±500 both have CoV=0.5\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping structural features (HPCSeries not available or missing dayofweek)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ train (after structural features) memory usage: 0.484 GB (shape=(3000888, 55))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.48350545577704906)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mem_gb(train, \"train (after structural features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.7. Promotion Features\n",
    "\n",
    "Promotions create sharp short-term lifts and delayed effects. Model promotion density, lags, and patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating promotion features...\n",
      "============================================================\n",
      "Analyzing promotion patterns (critical for retail forecasting)...\n",
      "\n",
      "âœ“ Created binary promotion flag (is_promo)\n",
      "\n",
      "Computing rolling promotion density...\n",
      "  [groups: 1782/1782] (100.0%)\n",
      "  âœ“ Computed rolling promo density for 1782 groups                    \n",
      "\n",
      "Computing lagged promotions...\n",
      "  âœ“ promo_lag_7\n",
      "  âœ“ promo_lag_14\n",
      "  âœ“ promo_lag_28\n",
      "\n",
      "âœ“ Created 7 promotion features in 5.0 seconds\n",
      "\n",
      " Promotion Features Summary:\n",
      "  Binary flag:             1 feature  (is_promo)\n",
      "  Rolling density:         3 features (7, 28, 56 day windows)\n",
      "  Lagged promotions:       3 features (7, 14, 28 day lags)\n",
      "\n",
      "ðŸ’¡ Why promotions matter:\n",
      "  - Create sharp short-term sales lifts (often nonlinear)\n",
      "  - Anticipatory buying BEFORE promos (people stock up)\n",
      "  - Delayed effects AFTER promos (stockpiling â†’ reduced sales)\n",
      "  - Promotion frequency affects baseline demand\n",
      "  - Cross-family interactions (promo on one product â†’ affects others)\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating promotion features...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "promo_cols = []\n",
    "start_promo = time.time()\n",
    "\n",
    "if 'onpromotion' in train.columns:\n",
    "    print(\"Analyzing promotion patterns (critical for retail forecasting)...\\n\")\n",
    "    \n",
    "    # Sort + reset index for clean NumPy indexing\n",
    "    train = train.sort_values(['store_nbr', 'family', 'date']).reset_index(drop=True)\n",
    "    n = len(train)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 1. Binary promotion flag\n",
    "    # ------------------------------------------------------------\n",
    "    train['is_promo'] = (train['onpromotion'] > 0).astype('int8')\n",
    "    promo_cols.append('is_promo')\n",
    "    print(\"âœ“ Created binary promotion flag (is_promo)\")\n",
    "    \n",
    "    # ------------------------------------------------------------\n",
    "    # 2. Rolling promotion density (per group)\n",
    "    # ------------------------------------------------------------\n",
    "    print(\"\\nComputing rolling promotion density...\")\n",
    "\n",
    "    # Preallocate arrays for rolling counts\n",
    "    promo_count_7  = np.full(n, np.nan, dtype=np.float32)\n",
    "    promo_count_28 = np.full(n, np.nan, dtype=np.float32)\n",
    "    promo_count_56 = np.full(n, np.nan, dtype=np.float32)\n",
    "\n",
    "    gb = train.groupby(['store_nbr', 'family'], sort=False)\n",
    "    n_groups = gb.ngroups\n",
    "\n",
    "    for i, (_, group) in enumerate(gb):\n",
    "        idx = group.index.to_numpy()\n",
    "        is_promo_vals = group['is_promo'].to_numpy(dtype=np.float64)\n",
    "\n",
    "        if is_promo_vals.size > 0:\n",
    "            try:\n",
    "                if HPCS_AVAILABLE:\n",
    "                    # Use HPCSeries rolling_sum for fast computation\n",
    "                    if is_promo_vals.size >= 7:\n",
    "                        promo_count_7[idx] = hpcs.rolling_sum(is_promo_vals, 7).astype(\n",
    "                            np.float32, copy=False\n",
    "                        )\n",
    "                    if is_promo_vals.size >= 28:\n",
    "                        promo_count_28[idx] = hpcs.rolling_sum(is_promo_vals, 28).astype(\n",
    "                            np.float32, copy=False\n",
    "                        )\n",
    "                    if is_promo_vals.size >= 56:\n",
    "                        promo_count_56[idx] = hpcs.rolling_sum(is_promo_vals, 56).astype(\n",
    "                            np.float32, copy=False\n",
    "                        )\n",
    "                else:\n",
    "                    # Fallback to pandas rolling; cast to float32\n",
    "                    s = group['is_promo']\n",
    "                    promo_count_7[idx] = (\n",
    "                        s.rolling(7, min_periods=1).sum().to_numpy(dtype=np.float32)\n",
    "                    )\n",
    "                    promo_count_28[idx] = (\n",
    "                        s.rolling(28, min_periods=1).sum().to_numpy(dtype=np.float32)\n",
    "                    )\n",
    "                    promo_count_56[idx] = (\n",
    "                        s.rolling(56, min_periods=1).sum().to_numpy(dtype=np.float32)\n",
    "                    )\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        if (i + 1) % 100 == 0 or (i + 1) == n_groups:\n",
    "            pct = 100.0 * (i + 1) / n_groups\n",
    "            print(f\"  [groups: {i+1:4d}/{n_groups}] ({pct:5.1f}%)\", end=\"\\r\")\n",
    "    \n",
    "    # Attach arrays to DataFrame once\n",
    "    train['promo_count_7']  = promo_count_7\n",
    "    train['promo_count_28'] = promo_count_28\n",
    "    train['promo_count_56'] = promo_count_56\n",
    "\n",
    "    promo_cols.extend(['promo_count_7', 'promo_count_28', 'promo_count_56'])\n",
    "    print(f\"\\n  âœ“ Computed rolling promo density for {n_groups} groups\" + \" \" * 20)\n",
    "    \n",
    "    # ------------------------------------------------------------\n",
    "    # 3. Lagged promotions (anticipatory buying & delayed effects)\n",
    "    # ------------------------------------------------------------\n",
    "    print(\"\\nComputing lagged promotions...\")\n",
    "\n",
    "    for lag in [7, 14, 28]:\n",
    "        col_name = f'promo_lag_{lag}'\n",
    "        train[col_name] = (\n",
    "            train\n",
    "            .groupby(['store_nbr', 'family'], sort=False)['onpromotion']\n",
    "            .shift(lag)\n",
    "            .astype('float32')\n",
    "        )\n",
    "        promo_cols.append(col_name)\n",
    "        print(f\"  âœ“ {col_name}\")\n",
    "    \n",
    "    elapsed_promo = time.time() - start_promo\n",
    "    print(f\"\\nâœ“ Created {len(promo_cols)} promotion features in {elapsed_promo:.1f} seconds\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n Promotion Features Summary:\")\n",
    "    print(f\"  Binary flag:             1 feature  (is_promo)\")\n",
    "    print(f\"  Rolling density:         3 features (7, 28, 56 day windows)\")\n",
    "    print(f\"  Lagged promotions:       3 features (7, 14, 28 day lags)\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ Why promotions matter:\")\n",
    "    print(f\"  - Create sharp short-term sales lifts (often nonlinear)\")\n",
    "    print(f\"  - Anticipatory buying BEFORE promos (people stock up)\")\n",
    "    print(f\"  - Delayed effects AFTER promos (stockpiling â†’ reduced sales)\")\n",
    "    print(f\"  - Promotion frequency affects baseline demand\")\n",
    "    print(f\"  - Cross-family interactions (promo on one product â†’ affects others)\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  'onpromotion' column not found - skipping promotion features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ train (after promo features) memory usage: 0.553 GB (shape=(3000888, 62))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.5533753242343664)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mem_gb(train, \"train (after promo features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.8. Holiday Features\n",
    "\n",
    "Process holidays_events.csv - handles transferred holidays, bridge days, and local vs national celebrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating holiday features...\n",
      "============================================================\n",
      "Processing holidays_events.csv...\n",
      "\n",
      "1. Categorizing holiday types...\n",
      "  âœ“ Processed 350 holiday events\n",
      "  âœ“ Found 449,064 holiday rows in training data\n",
      "    - National: 254,826\n",
      "    - Local:    212,058\n",
      "\n",
      "2. Computing holiday lags and leads...\n",
      "  âœ“ Created holiday lags (1â€“3 days before) and leads (1â€“3 days after)\n",
      "\n",
      "3. Computing holiday block density...\n",
      "  âœ“ Computed holiday density using HPCSeries rolling_sum\n",
      "\n",
      "âœ“ Created 15 holiday features in 8.5 seconds\n",
      "\n",
      " Holiday Features Summary:\n",
      "  Type flags:              7 features (holiday, transferred, bridge, workday, etc.)\n",
      "  Lags & Leads:            6 features (Â±1â€“3 days)\n",
      "  Holiday blocks:          2 features (7, 14 day density)\n",
      "\n",
      "ðŸ’¡ Why holiday features are critical:\n",
      "  - People buy MORE before holidays (gift shopping, food prep)\n",
      "  - People buy LESS after holidays (stockpiling effect)\n",
      "  - Transferred holidays behave differently than actual date\n",
      "  - Bridge days extend holiday effects across long weekends\n",
      "  - Work Days compensate for bridges (normal shopping patterns)\n",
      "  - Local holidays only affect specific regions (city/state specific)\n",
      "  - This dataset is what separates top solutions from average ones!\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating holiday features...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "holiday_cols = []\n",
    "start_holiday = time.time()\n",
    "\n",
    "if 'holidays' in locals() and holidays is not None and len(holidays) > 0:\n",
    "    print(\"Processing holidays_events.csv...\\n\")\n",
    "    \n",
    "    n = len(train)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 1. Type flags\n",
    "    # ------------------------------------------------------------\n",
    "    print(\"1. Categorizing holiday types...\")\n",
    "\n",
    "    # Initialise all holiday flags\n",
    "    train['is_holiday']             = np.zeros(n, dtype=np.int8)\n",
    "    train['is_transferred_holiday'] = np.zeros(n, dtype=np.int8)\n",
    "    train['is_bridge_day']          = np.zeros(n, dtype=np.int8)\n",
    "    train['is_workday']             = np.zeros(n, dtype=np.int8)\n",
    "    train['is_additional_holiday']  = np.zeros(n, dtype=np.int8)\n",
    "    train['is_local_holiday']       = np.zeros(n, dtype=np.int8)\n",
    "    train['is_national_holiday']    = np.zeros(n, dtype=np.int8)\n",
    "\n",
    "    for _, row in holidays.iterrows():\n",
    "        date         = row['date']\n",
    "        holiday_type = row['type']\n",
    "        transferred  = row.get('transferred', False)\n",
    "        locale       = row.get('locale', 'National')\n",
    "        locale_name  = row.get('locale_name', None)\n",
    "        \n",
    "        # Match dates in train\n",
    "        date_mask = (train['date'] == date)\n",
    "        \n",
    "        if date_mask.any():\n",
    "            # Base holiday flag\n",
    "            train.loc[date_mask, 'is_holiday'] = 1\n",
    "            \n",
    "            # Type-specific flags\n",
    "            if bool(transferred):\n",
    "                train.loc[date_mask, 'is_transferred_holiday'] = 1\n",
    "            \n",
    "            if holiday_type == 'Bridge':\n",
    "                train.loc[date_mask, 'is_bridge_day'] = 1\n",
    "            elif holiday_type == 'Work Day':\n",
    "                train.loc[date_mask, 'is_workday'] = 1\n",
    "            elif holiday_type == 'Additional':\n",
    "                train.loc[date_mask, 'is_additional_holiday'] = 1\n",
    "            \n",
    "            # Local vs National\n",
    "            if locale == 'National':\n",
    "                train.loc[date_mask, 'is_national_holiday'] = 1\n",
    "            else:\n",
    "                # Local holiday - match by state/city if available\n",
    "                train.loc[date_mask, 'is_local_holiday'] = 1\n",
    "                if locale_name and 'state' in train.columns:\n",
    "                    state_mask = (train['state'] == locale_name)\n",
    "                    train.loc[date_mask & state_mask, 'is_local_holiday'] = 1\n",
    "    \n",
    "    holiday_cols.extend([\n",
    "        'is_holiday',\n",
    "        'is_transferred_holiday',\n",
    "        'is_bridge_day',\n",
    "        'is_workday',\n",
    "        'is_additional_holiday',\n",
    "        'is_local_holiday',\n",
    "        'is_national_holiday',\n",
    "    ])\n",
    "    \n",
    "    n_holidays = int(train['is_holiday'].sum())\n",
    "    n_national = int(train['is_national_holiday'].sum())\n",
    "    n_local    = int(train['is_local_holiday'].sum())\n",
    "    print(f\"  âœ“ Processed {len(holidays)} holiday events\")\n",
    "    print(f\"  âœ“ Found {n_holidays:,} holiday rows in training data\")\n",
    "    print(f\"    - National: {n_national:,}\")\n",
    "    print(f\"    - Local:    {n_local:,}\")\n",
    "    \n",
    "    # ------------------------------------------------------------\n",
    "    # 2. Holiday lag/lead features (people buy before/after holidays)\n",
    "    # ------------------------------------------------------------\n",
    "    print(\"\\n2. Computing holiday lags and leads...\")\n",
    "\n",
    "    # Sort once by date for global lag/lead (we'll re-sort by store/family later if needed)\n",
    "    train = train.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "    for lag in [1, 2, 3]:\n",
    "        lag_col  = f'holiday_lag_{lag}'\n",
    "        lead_col = f'holiday_lead_{lag}'\n",
    "\n",
    "        lag_vals = train['is_holiday'].shift(lag).fillna(0).astype('int8')\n",
    "        lead_vals = train['is_holiday'].shift(-lag).fillna(0).astype('int8')\n",
    "\n",
    "        train[lag_col]  = lag_vals\n",
    "        train[lead_col] = lead_vals\n",
    "\n",
    "        holiday_cols.extend([lag_col, lead_col])\n",
    "\n",
    "    print(\"  âœ“ Created holiday lags (1â€“3 days before) and leads (1â€“3 days after)\")\n",
    "    \n",
    "    # ------------------------------------------------------------\n",
    "    # 3. Holiday blocks (rolling density over time)\n",
    "    # ------------------------------------------------------------\n",
    "    print(\"\\n3. Computing holiday block density...\")\n",
    "\n",
    "    holiday_array = train['is_holiday'].to_numpy(dtype=np.float64)\n",
    "\n",
    "    if HPCS_AVAILABLE and holiday_array.size >= 7:\n",
    "        try:\n",
    "            block_7 = hpcs.rolling_sum(holiday_array, 7).astype(np.float32, copy=False)\n",
    "            train['holiday_block_7'] = block_7\n",
    "\n",
    "            if holiday_array.size >= 14:\n",
    "                block_14 = hpcs.rolling_sum(holiday_array, 14).astype(np.float32, copy=False)\n",
    "                train['holiday_block_14'] = block_14\n",
    "\n",
    "            holiday_cols.extend(['holiday_block_7', 'holiday_block_14'])\n",
    "            print(\"  âœ“ Computed holiday density using HPCSeries rolling_sum\")\n",
    "        except Exception:\n",
    "            # Pandas fallback\n",
    "            train['holiday_block_7'] = (\n",
    "                train['is_holiday'].rolling(7, min_periods=1).sum().astype('float32')\n",
    "            )\n",
    "            train['holiday_block_14'] = (\n",
    "                train['is_holiday'].rolling(14, min_periods=1).sum().astype('float32')\n",
    "            )\n",
    "            holiday_cols.extend(['holiday_block_7', 'holiday_block_14'])\n",
    "            print(\"  âœ“ Computed holiday density using pandas\")\n",
    "    else:\n",
    "        train['holiday_block_7'] = (\n",
    "            train['is_holiday'].rolling(7, min_periods=1).sum().astype('float32')\n",
    "        )\n",
    "        train['holiday_block_14'] = (\n",
    "            train['is_holiday'].rolling(14, min_periods=1).sum().astype('float32')\n",
    "        )\n",
    "        holiday_cols.extend(['holiday_block_7', 'holiday_block_14'])\n",
    "        print(\"  âœ“ Computed holiday density using pandas\")\n",
    "    \n",
    "    elapsed_holiday = time.time() - start_holiday\n",
    "    print(f\"\\nâœ“ Created {len(holiday_cols)} holiday features in {elapsed_holiday:.1f} seconds\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n Holiday Features Summary:\")\n",
    "    print(f\"  Type flags:              7 features (holiday, transferred, bridge, workday, etc.)\")\n",
    "    print(f\"  Lags & Leads:            6 features (Â±1â€“3 days)\")\n",
    "    print(f\"  Holiday blocks:          2 features (7, 14 day density)\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ Why holiday features are critical:\")\n",
    "    print(f\"  - People buy MORE before holidays (gift shopping, food prep)\")\n",
    "    print(f\"  - People buy LESS after holidays (stockpiling effect)\")\n",
    "    print(f\"  - Transferred holidays behave differently than actual date\")\n",
    "    print(f\"  - Bridge days extend holiday effects across long weekends\")\n",
    "    print(f\"  - Work Days compensate for bridges (normal shopping patterns)\")\n",
    "    print(f\"  - Local holidays only affect specific regions (city/state specific)\")\n",
    "    print(f\"  - This dataset is what separates top solutions from average ones!\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  Holidays data not loaded - skipping holiday features\")\n",
    "    print(\"   Make sure 'holidays' DataFrame is available from earlier data loading\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.9. Domain-Specific Events (Payday & Earthquake) \n",
    "\n",
    "Ecuador-specific events: public sector paydays (15th & last day) and 2016 earthquake relief-buying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating domain-specific event features...\n",
      "============================================================\n",
      "Processing Ecuador-specific events (payday & earthquake)...\n",
      "\n",
      "1. Creating payday features...\n",
      "   (Public sector wages paid on 15th and last day of month)\n",
      "  âœ“ Created payday features\n",
      "    - Payday dates: 197,802\n",
      "    - Payday windows (Â±3 days): 1,384,614\n",
      "\n",
      "2. Creating earthquake features...\n",
      "   (Magnitude 7.8 earthquake on 2016-04-16)\n",
      "  âœ“ Created earthquake features\n",
      "    - Earthquake date: 2016-04-16\n",
      "    - Days affected (21-day window): 37,422 rows\n",
      "\n",
      "âœ“ Created 8 event features in 1.1 seconds\n",
      "\n",
      " Event Features: 4 payday + 4 earthquake = 8 features\n",
      "âš¡ Memory optimized: Using int8/int16/float32 dtypes + garbage collection\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating domain-specific event features...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "event_cols = []\n",
    "start_event = time.time()\n",
    "\n",
    "if 'date' in train.columns:\n",
    "    print(\"Processing Ecuador-specific events (payday & earthquake)...\\n\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # PAYDAY FEATURES - MEMORY EFFICIENT\n",
    "    # ============================================================\n",
    "    print(\"1. Creating payday features...\")\n",
    "    print(\"   (Public sector wages paid on 15th and last day of month)\")\n",
    "    \n",
    "    \n",
    "    # Extract day as numpy array\n",
    "    day_vals = train['date'].dt.day.values\n",
    "    month_end_vals = train['date'].dt.days_in_month.values\n",
    "    \n",
    "    is_payday = ((day_vals == 15) | (day_vals == month_end_vals)).astype(np.int8)\n",
    "    train['is_payday'] = is_payday\n",
    "    del is_payday\n",
    "    gc.collect()\n",
    "    \n",
    "    # Days to payday\n",
    "    days_to = np.where(day_vals < 15, 15 - day_vals,\n",
    "                       np.where(day_vals == 15, 0, month_end_vals - day_vals))\n",
    "    train['days_to_payday'] = days_to.astype(np.int8)\n",
    "    del days_to\n",
    "    gc.collect()\n",
    "    \n",
    "    # Days after payday\n",
    "    days_after = np.where(day_vals <= 15, day_vals, day_vals - 15)\n",
    "    train['days_after_payday'] = days_after.astype(np.int8)\n",
    "    del days_after\n",
    "    gc.collect()\n",
    "    \n",
    "    # Payday window\n",
    "    train['payday_window_3'] = ((train['days_to_payday'] <= 3) | \n",
    "                                 (train['days_after_payday'] <= 3)).astype(np.int8)\n",
    "    \n",
    "    del day_vals, month_end_vals \n",
    "    gc.collect()\n",
    "    \n",
    "    event_cols.extend(['is_payday', 'days_to_payday', 'days_after_payday', 'payday_window_3'])\n",
    "    \n",
    "    n_paydays = train['is_payday'].sum()\n",
    "    n_payday_windows = train['payday_window_3'].sum()\n",
    "    print(f\"  âœ“ Created payday features\")\n",
    "    print(f\"    - Payday dates: {n_paydays:,}\")\n",
    "    print(f\"    - Payday windows (Â±3 days): {n_payday_windows:,}\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # EARTHQUAKE FEATURES - MEMORY EFFICIENT\n",
    "    # ============================================================\n",
    "    print(\"\\n2. Creating earthquake features...\")\n",
    "    print(\"   (Magnitude 7.8 earthquake on 2016-04-16)\")\n",
    "    \n",
    "    earthquake_date = pd.Timestamp('2016-04-16')\n",
    "    \n",
    "    # Is earthquake day?\n",
    "    train['is_earthquake'] = (train['date'] == earthquake_date).astype(np.int8)\n",
    "    \n",
    "    # Days after earthquake\n",
    "    days_diff = (train['date'] - earthquake_date).dt.days.values\n",
    "    days_diff = np.clip(days_diff, 0, None).astype(np.int16)  # int16 is enough\n",
    "    train['days_after_eq'] = days_diff\n",
    "    \n",
    "    # Earthquake window\n",
    "    train['eq_window_21'] = ((days_diff > 0) & (days_diff <= 21)).astype(np.int8)\n",
    "    \n",
    "    # Earthquake effect\n",
    "    eq_effect = np.where(days_diff > 0, np.exp(-days_diff / 10.0), 0.0).astype(np.float32)\n",
    "    train['eq_effect'] = eq_effect\n",
    "    \n",
    "    # Clean up\n",
    "    del days_diff, eq_effect\n",
    "    gc.collect()\n",
    "    \n",
    "    event_cols.extend(['is_earthquake', 'days_after_eq', 'eq_window_21', 'eq_effect'])\n",
    "    \n",
    "    n_eq_affected = train['eq_window_21'].sum()\n",
    "    print(f\"  âœ“ Created earthquake features\")\n",
    "    print(f\"    - Earthquake date: 2016-04-16\")\n",
    "    print(f\"    - Days affected (21-day window): {n_eq_affected:,} rows\")\n",
    "    \n",
    "    elapsed_event = time.time() - start_event\n",
    "    print(f\"\\nâœ“ Created {len(event_cols)} event features in {elapsed_event:.1f} seconds\")\n",
    "    \n",
    "    print(f\"\\n Event Features: 4 payday + 4 earthquake = 8 features\")\n",
    "    print(f\"âš¡ Memory optimized: Using int8/int16/float32 dtypes + garbage collection\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  'date' column not found - skipping event features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ train (payday and earthquakes) memory usage: 0.646 GB (shape=(3000888, 85))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.6456035505980253)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mem_gb(train, \"train (payday and earthquakes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.10. Oil Price Features\n",
    "\n",
    "Ecuador is oil-dependent - oil price affects government budgets and consumer spending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating oil price features...\n",
      "============================================================\n",
      "Analyzing oil price patterns (Ecuador is oil-dependent)...\n",
      "\n",
      "1. Computing rolling oil price averages...\n",
      "  âœ“ Computed rolling means using HPCSeries\n",
      "\n",
      "2. Computing oil price percent changes...\n",
      "  âœ“ Computed 7-day percent change\n",
      "\n",
      "3. Computing oil price volatility (MAD)...\n",
      "  âœ“ Computed rolling MAD using HPCSeries (robust volatility)\n",
      "\n",
      "âœ“ Created 4 oil price features in 4.0 seconds\n",
      "\n",
      " Oil Price Features Summary:\n",
      "  Rolling averages:        2 features (7, 28 day windows)\n",
      "  Percent change:          1 feature  (7 day)\n",
      "  Volatility (MAD/std):    1 feature  (28 day window)\n",
      "\n",
      "ðŸ’¡ Why oil prices matter for Ecuador:\n",
      "  - Oil-dependent economy: Government budgets tied to oil revenue\n",
      "  - Consumer spending: Oil price drops â†’ reduced wages â†’ lower sales\n",
      "  - Oil price rises: Boost in spending power\n",
      "  - Smooth indicators: Rolling means capture medium-term trends\n",
      "  - Volatility: High oil volatility = economic uncertainty\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating oil price features...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "oil_cols = []\n",
    "start_oil = time.time()\n",
    "\n",
    "if 'dcoilwtico' in train.columns:\n",
    "    print(\"Analyzing oil price patterns (Ecuador is oil-dependent)...\\n\")\n",
    "    \n",
    "    # Sort by date for proper temporal rolling\n",
    "    train = train.sort_values('date').reset_index(drop=True)\n",
    "    n = len(train)\n",
    "    \n",
    "    # Base series as float32 (storage)\n",
    "    dcoil = train['dcoilwtico'].astype('float32')\n",
    "    train['dcoilwtico'] = dcoil \n",
    "    \n",
    "    oil_prices = dcoil.to_numpy(dtype=np.float64)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 1. Rolling oil price averages\n",
    "    # ------------------------------------------------------------\n",
    "    print(\"1. Computing rolling oil price averages...\")\n",
    "    if HPCS_AVAILABLE and oil_prices.size > 0:\n",
    "        try:\n",
    "            if oil_prices.size >= 7:\n",
    "                roll_7 = hpcs.rolling_mean(oil_prices, 7).astype(np.float32, copy=False)\n",
    "                train['oil_roll_mean_7'] = roll_7\n",
    "                oil_cols.append('oil_roll_mean_7')\n",
    "            if oil_prices.size >= 28:\n",
    "                roll_28 = hpcs.rolling_mean(oil_prices, 28).astype(np.float32, copy=False)\n",
    "                train['oil_roll_mean_28'] = roll_28\n",
    "                oil_cols.append('oil_roll_mean_28')\n",
    "            print(\"  âœ“ Computed rolling means using HPCSeries\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸  HPCSeries failed ({e}), using pandas fallback\")\n",
    "            train['oil_roll_mean_7'] = (\n",
    "                dcoil.rolling(7, min_periods=1).mean().astype('float32')\n",
    "            )\n",
    "            train['oil_roll_mean_28'] = (\n",
    "                dcoil.rolling(28, min_periods=1).mean().astype('float32')\n",
    "            )\n",
    "            if 'oil_roll_mean_7' not in oil_cols:\n",
    "                oil_cols.extend(['oil_roll_mean_7', 'oil_roll_mean_28'])\n",
    "    else:\n",
    "        # Pandas fallback\n",
    "        train['oil_roll_mean_7'] = (\n",
    "            dcoil.rolling(7, min_periods=1).mean().astype('float32')\n",
    "        )\n",
    "        train['oil_roll_mean_28'] = (\n",
    "            dcoil.rolling(28, min_periods=1).mean().astype('float32')\n",
    "        )\n",
    "        oil_cols.extend(['oil_roll_mean_7', 'oil_roll_mean_28'])\n",
    "        print(\"  âœ“ Computed rolling means using pandas\")\n",
    "    \n",
    "    # ------------------------------------------------------------\n",
    "    # 2. Percent change (7-day)\n",
    "    # ------------------------------------------------------------\n",
    "    print(\"\\n2. Computing oil price percent changes...\")\n",
    "    oil_pct_7 = dcoil.pct_change(periods=7).fillna(0.0).astype('float32')\n",
    "    train['oil_pct_change_7'] = oil_pct_7\n",
    "    oil_cols.append('oil_pct_change_7')\n",
    "    print(\"  âœ“ Computed 7-day percent change\")\n",
    "    \n",
    "    # ------------------------------------------------------------\n",
    "    # 3. Volatility (MAD or std) over 28 days\n",
    "    # ------------------------------------------------------------\n",
    "    print(\"\\n3. Computing oil price volatility (MAD)...\")\n",
    "    if HPCS_AVAILABLE and oil_prices.size >= 28:\n",
    "        try:\n",
    "            vol_28 = hpcs.rolling_mad(oil_prices, 28).astype(np.float32, copy=False)\n",
    "            train['oil_volatility_28'] = vol_28\n",
    "            oil_cols.append('oil_volatility_28')\n",
    "            print(\"  âœ“ Computed rolling MAD using HPCSeries (robust volatility)\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸  HPCSeries MAD failed ({e}), using std fallback\")\n",
    "            train['oil_volatility_28'] = (\n",
    "                dcoil.rolling(28, min_periods=1).std().astype('float32')\n",
    "            )\n",
    "            oil_cols.append('oil_volatility_28')\n",
    "    else:\n",
    "        train['oil_volatility_28'] = (\n",
    "            dcoil.rolling(28, min_periods=1).std().astype('float32')\n",
    "        )\n",
    "        oil_cols.append('oil_volatility_28')\n",
    "        print(\"  âœ“ Computed rolling std using pandas\")\n",
    "    \n",
    "    elapsed_oil = time.time() - start_oil\n",
    "    print(f\"\\nâœ“ Created {len(oil_cols)} oil price features in {elapsed_oil:.1f} seconds\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n Oil Price Features Summary:\")\n",
    "    print(f\"  Rolling averages:        2 features (7, 28 day windows)\")\n",
    "    print(f\"  Percent change:          1 feature  (7 day)\")\n",
    "    print(f\"  Volatility (MAD/std):    1 feature  (28 day window)\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ Why oil prices matter for Ecuador:\")\n",
    "    print(f\"  - Oil-dependent economy: Government budgets tied to oil revenue\")\n",
    "    print(f\"  - Consumer spending: Oil price drops â†’ reduced wages â†’ lower sales\")\n",
    "    print(f\"  - Oil price rises: Boost in spending power\")\n",
    "    print(f\"  - Smooth indicators: Rolling means capture medium-term trends\")\n",
    "    print(f\"  - Volatility: High oil volatility = economic uncertainty\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  'dcoilwtico' column not found - skipping oil price features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ train (Oil features) memory usage: 0.690 GB (shape=(3000888, 89))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.6903202664107084)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mem_gb(train, \"train (Oil features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.11. Final Feature Engineering\n",
    "\n",
    "Combine all feature blocks and prepare the final feature set for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FEATURE ENGINEERING\n",
      "============================================================\n",
      "\n",
      "Combining ALL feature blocks into final feature set...\n",
      "\n",
      "âœ“ Encoded 5 categorical features\n",
      "âœ“ Base features: 21\n",
      "âœ“ Block A - Rolling stats:            12 features (windows: 7, 28, 56)\n",
      "âœ“ Block A - Derived ratios:            9 features (MAD/median, STD/MAD, volatility index)\n",
      "âœ“ Block B - Robust anomaly detection:  9 features (global z-score, clipped, flags, density)\n",
      "âœ“ Block C - Structural features:       2 features (seasonality strength, CoV)\n",
      "âœ“ Block B - Promotion features:        7 features (density, lags)\n",
      "âœ“ Block C - Holiday features:         15 features (types, lags, blocks)\n",
      "âœ“ Block D/E - Event features:          8 features (payday, earthquake)\n",
      "âœ“ Block F - Oil price features:        4 features (rolling, pct change, volatility)\n",
      "\n",
      "Total feature columns (deduplicated): 87\n",
      "Using 87 features present in train\n",
      "\n",
      "============================================================\n",
      "Missing-value diagnostics (no DataFrame-wide imputation)\n",
      "============================================================\n",
      "\n",
      "âš ï¸  There are 10,002,926 missing values across 25 features.\n",
      "\n",
      "Top 10 features by missing count:\n",
      "oil_roll_mean_7          2993760\n",
      "oil_roll_mean_28         2993760\n",
      "oil_volatility_28         863676\n",
      "dcoilwtico                857142\n",
      "robust_zscore_clipped     588703\n",
      "robust_zscore_global      588703\n",
      "transactions              245784\n",
      "sales_roll_mad_56          98010\n",
      "sales_roll_median_56       98010\n",
      "mad_over_median_56         98010\n",
      "dtype: int64\n",
      "\n",
      "âœ“ Column-wise median imputation completed\n",
      "\n",
      "============================================================\n",
      "FEATURE ENGINEERING COMPLETE âœ…\n",
      "============================================================\n",
      "\n",
      " FINAL FEATURE SUMMARY:\n",
      "\n",
      "============================================================\n",
      "Category                            Count\n",
      "============================================================\n",
      "Time features                          12\n",
      "Categorical features                    5\n",
      "Other base features                     4\n",
      "------------------------------------------------------------\n",
      "HPCSeries Core Features:               32\n",
      "  - Rolling stats (7/28/56)            12\n",
      "  - Derived ratios                      9\n",
      "  - Robust anomaly detection            9\n",
      "  - Structural (seasonality, CoV)       2\n",
      "------------------------------------------------------------\n",
      "Domain-Specific Features:              34\n",
      "  - Promotions                          7\n",
      "  - Holidays                           15\n",
      "  - Events (payday, earthquake)         8\n",
      "  - Oil prices                          4\n",
      "============================================================\n",
      "TOTAL FEATURES                         87\n",
      "============================================================\n",
      "\n",
      "ðŸš€ Performance notes:\n",
      "  - HPCSeries SIMD operations:  32 features\n",
      "  - Domain expertise modeling:  34 features\n",
      "  - No DataFrame-wide imputation: XGBoost will treat NaNs natively\n",
      "  - Parallelization:            handled by XGBoost at training time\n",
      "ðŸ“¦ train (final features) memory usage: 0.710 GB (shape=(3000888, 94))\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nCombining ALL feature blocks into final feature set...\\n\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Encode categorical variables with compact dtypes\n",
    "# ------------------------------------------------------------\n",
    "train['family_enc']  = train['family'].astype('category').cat.codes.astype('int16')\n",
    "train['type_enc']    = train['type'].astype('category').cat.codes.astype('int8')\n",
    "train['city_enc']    = train['city'].astype('category').cat.codes.astype('int16')\n",
    "train['state_enc']   = train['state'].astype('category').cat.codes.astype('int8')\n",
    "\n",
    "train['cluster_enc'] = train['cluster'].astype('int8')\n",
    "\n",
    "categorical_features = ['family_enc', 'type_enc', 'city_enc', 'state_enc', 'cluster_enc']\n",
    "print(f\"âœ“ Encoded {len(categorical_features)} categorical features\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Base features\n",
    "# ------------------------------------------------------------\n",
    "feature_cols = (\n",
    "    ['store_nbr', 'onpromotion'] +\n",
    "    time_features +\n",
    "    ['dcoilwtico', 'transactions'] +\n",
    "    categorical_features\n",
    ")\n",
    "\n",
    "base_count = len(feature_cols)\n",
    "print(f\"âœ“ Base features: {base_count}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Add all HPCSeries feature blocks\n",
    "# ------------------------------------------------------------\n",
    "hpcs_feature_count = 0\n",
    "\n",
    "if 'rolling_cols' in locals() and len(rolling_cols) > 0:\n",
    "    feature_cols += rolling_cols\n",
    "    hpcs_feature_count += len(rolling_cols)\n",
    "    print(f\"âœ“ Block A - Rolling stats:           {len(rolling_cols):>3} features (windows: 7, 28, 56)\")\n",
    "\n",
    "if 'derived_cols' in locals() and len(derived_cols) > 0:\n",
    "    feature_cols += derived_cols\n",
    "    hpcs_feature_count += len(derived_cols)\n",
    "    print(f\"âœ“ Block A - Derived ratios:          {len(derived_cols):>3} features (MAD/median, STD/MAD, volatility index)\")\n",
    "\n",
    "if 'advanced_cols' in locals() and len(advanced_cols) > 0:\n",
    "    feature_cols += advanced_cols\n",
    "    hpcs_feature_count += len(advanced_cols)\n",
    "    print(f\"âœ“ Block B - Robust anomaly detection:{len(advanced_cols):>3} features (global z-score, clipped, flags, density)\")\n",
    "\n",
    "if 'structural_cols' in locals() and len(structural_cols) > 0:\n",
    "    feature_cols += structural_cols\n",
    "    hpcs_feature_count += len(structural_cols)\n",
    "    print(f\"âœ“ Block C - Structural features:     {len(structural_cols):>3} features (seasonality strength, CoV)\")\n",
    "\n",
    "if 'lag_cols' in locals() and len(lag_cols) > 0:\n",
    "    feature_cols += lag_cols\n",
    "    hpcs_feature_count += len(lag_cols)\n",
    "    print(f\"âœ“ Lag features:                       {len(lag_cols):>3} features\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Add domain-specific feature blocks\n",
    "# ------------------------------------------------------------\n",
    "domain_feature_count = 0\n",
    "\n",
    "if 'promo_cols' in locals() and len(promo_cols) > 0:\n",
    "    feature_cols += promo_cols\n",
    "    domain_feature_count += len(promo_cols)\n",
    "    print(f\"âœ“ Block B - Promotion features:      {len(promo_cols):>3} features (density, lags)\")\n",
    "\n",
    "if 'holiday_cols' in locals() and len(holiday_cols) > 0:\n",
    "    feature_cols += holiday_cols\n",
    "    domain_feature_count += len(holiday_cols)\n",
    "    print(f\"âœ“ Block C - Holiday features:        {len(holiday_cols):>3} features (types, lags, blocks)\")\n",
    "\n",
    "if 'event_cols' in locals() and len(event_cols) > 0:\n",
    "    feature_cols += event_cols\n",
    "    domain_feature_count += len(event_cols)\n",
    "    print(f\"âœ“ Block D/E - Event features:        {len(event_cols):>3} features (payday, earthquake)\")\n",
    "\n",
    "if 'oil_cols' in locals() and len(oil_cols) > 0:\n",
    "    feature_cols += oil_cols\n",
    "    domain_feature_count += len(oil_cols)\n",
    "    print(f\"âœ“ Block F - Oil price features:      {len(oil_cols):>3} features (rolling, pct change, volatility)\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Deduplicate feature list while preserving order\n",
    "# ------------------------------------------------------------\n",
    "seen = set()\n",
    "feature_cols = [c for c in feature_cols if not (c in seen or seen.add(c))]\n",
    "\n",
    "print(f\"\\nTotal feature columns (deduplicated): {len(feature_cols)}\")\n",
    "\n",
    "# Only keep features that actually exist\n",
    "feature_cols_existing = [c for c in feature_cols if c in train.columns]\n",
    "missing_features = sorted(set(feature_cols) - set(feature_cols_existing))\n",
    "if missing_features:\n",
    "    print(f\"\\nâš ï¸  {len(missing_features)} requested features not found in train (showing up to 10):\")\n",
    "    print(\"   \", missing_features[:10])\n",
    "\n",
    "print(f\"Using {len(feature_cols_existing)} features present in train\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) Missing-value diagnostics\n",
    "# ------------------------------------------------------------\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Missing-value diagnostics (no DataFrame-wide imputation)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "numeric_cols = train[feature_cols_existing].select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "missing_summary = train[feature_cols_existing].isnull().sum()\n",
    "total_missing = int(missing_summary.sum())\n",
    "\n",
    "if total_missing > 0:\n",
    "    print(f\"âš ï¸  There are {total_missing:,} missing values across \"\n",
    "          f\"{(missing_summary > 0).sum()} features.\")\n",
    "    print(\"\\nTop 10 features by missing count:\")\n",
    "    print(missing_summary.sort_values(ascending=False).head(10))\n",
    "else:\n",
    "    print(\"âœ… No missing values in selected features\")\n",
    "\n",
    "# OPTIONAL: If you REALLY want median imputation, do it column-by-column.\n",
    "# This is much safer than train[numeric_cols] = train[numeric_cols].fillna(medians)\n",
    "# and should not explode memory:\n",
    "#\n",
    "for col in numeric_cols:\n",
    "     if train[col].isnull().any():\n",
    "         med = train[col].median()\n",
    "         if pd.notnull(med):\n",
    "             train[col].fillna(med, inplace=True)\n",
    "\n",
    "print(\"\\nâœ“ Column-wise median imputation completed\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FEATURE ENGINEERING COMPLETE âœ…\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(f\" FINAL FEATURE SUMMARY:\\n\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"{'Category':<35} {'Count':>5}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"{'Time features':<35} {len(time_features):>5}\")\n",
    "print(f\"{'Categorical features':<35} {len(categorical_features):>5}\")\n",
    "print(f\"{'Other base features':<35} {base_count - len(time_features) - len(categorical_features):>5}\")\n",
    "print(f\"{'-'*60}\")\n",
    "print(f\"{'HPCSeries Core Features:':<35} {hpcs_feature_count:>5}\")\n",
    "if 'rolling_cols' in locals():\n",
    "    print(f\"{'  - Rolling stats (7/28/56)':<35} {len(rolling_cols):>5}\")\n",
    "if 'derived_cols' in locals():\n",
    "    print(f\"{'  - Derived ratios':<35} {len(derived_cols):>5}\")\n",
    "if 'advanced_cols' in locals():\n",
    "    print(f\"{'  - Robust anomaly detection':<35} {len(advanced_cols):>5}\")\n",
    "if 'structural_cols' in locals():\n",
    "    print(f\"{'  - Structural (seasonality, CoV)':<35} {len(structural_cols):>5}\")\n",
    "if 'lag_cols' in locals():\n",
    "    print(f\"{'  - Lag features':<35} {len(lag_cols):>5}\")\n",
    "print(f\"{'-'*60}\")\n",
    "print(f\"{'Domain-Specific Features:':<35} {domain_feature_count:>5}\")\n",
    "if 'promo_cols' in locals():\n",
    "    print(f\"{'  - Promotions':<35} {len(promo_cols):>5}\")\n",
    "if 'holiday_cols' in locals():\n",
    "    print(f\"{'  - Holidays':<35} {len(holiday_cols):>5}\")\n",
    "if 'event_cols' in locals():\n",
    "    print(f\"{'  - Events (payday, earthquake)':<35} {len(event_cols):>5}\")\n",
    "if 'oil_cols' in locals():\n",
    "    print(f\"{'  - Oil prices':<35} {len(oil_cols):>5}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"{'TOTAL FEATURES':<35} {len(feature_cols_existing):>5}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(\"ðŸš€ Performance notes:\")\n",
    "print(f\"  - HPCSeries SIMD operations:  {hpcs_feature_count} features\")\n",
    "print(f\"  - Domain expertise modeling:  {domain_feature_count} features\")\n",
    "print(f\"  - No DataFrame-wide imputation: XGBoost will treat NaNs natively\")\n",
    "print(f\"  - Parallelization:            handled by XGBoost at training time\")\n",
    "\n",
    "\n",
    "if 'df_mem_gb' in globals():\n",
    "    df_mem_gb(train, \"train (final features)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Lag Features\n",
    "\n",
    "Add historical sales values as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating lag features...\n",
      "============================================================\n",
      "  âœ“ sales_lag_7\n",
      "  âœ“ sales_lag_14\n",
      "  âœ“ sales_lag_28\n",
      "\n",
      "âœ“ Created 3 lag features\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating lag features...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sort once to guarantee correct lag alignment\n",
    "train = train.sort_values(['store_nbr', 'family', 'date'], ignore_index=True)\n",
    "\n",
    "lag_cols = []\n",
    "\n",
    "for lag in [7, 14, 28]:\n",
    "    col_name = f'sales_lag_{lag}'\n",
    "    \n",
    "    # Shift within each store-family group\n",
    "    lagged = (\n",
    "        train.groupby(['store_nbr', 'family'])['sales']\n",
    "             .shift(lag)\n",
    "             .astype('float32')  \n",
    "    )\n",
    "    \n",
    "    train[col_name] = lagged\n",
    "    lag_cols.append(col_name)\n",
    "    \n",
    "    print(f\"  âœ“ {col_name}\")\n",
    "\n",
    "print(f\"\\nâœ“ Created {len(lag_cols)} lag features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Feature Engineering\n",
    "\n",
    "Prepare final feature set for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing features for XGBoost...\n",
      "============================================================\n",
      "âœ… Using 87 final features for modelling\n",
      "\n",
      " Feature Breakdown:\n",
      "============================================================\n",
      "Category                            Count\n",
      "============================================================\n",
      "Time features                          12\n",
      "Categorical features                    5\n",
      "Other base features                     4\n",
      "------------------------------------------------------------\n",
      "HPCSeries Core Features:               35\n",
      "  - Rolling stats (7/28/56)            12\n",
      "  - Derived ratios                      9\n",
      "  - Robust anomaly detection            9\n",
      "  - Structural (seasonality, CoV)       2\n",
      "  - Lag features                        3\n",
      "------------------------------------------------------------\n",
      "Domain-Specific Features:              34\n",
      "  - Promotions                          7\n",
      "  - Holidays                           15\n",
      "  - Events (payday, earthquake)         8\n",
      "  - Oil prices                          4\n",
      "============================================================\n",
      "TOTAL FEATURES                         87\n",
      "============================================================\n",
      "\n",
      "Ready for modelling: use `feature_cols` as your X columns\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing features for XGBoost...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Reuse final engineered feature list from Cell 5.11\n",
    "# ------------------------------------------------------------\n",
    "if 'feature_cols_existing' in globals():\n",
    "    feature_cols = feature_cols_existing\n",
    "else:\n",
    "    print(\"âš ï¸ feature_cols_existing not found â€“ rebuilding feature list from blocks\")\n",
    "    feature_cols = (\n",
    "        ['store_nbr', 'onpromotion'] +\n",
    "        time_features +\n",
    "        ['dcoilwtico', 'transactions'] +\n",
    "        categorical_features\n",
    "    )\n",
    "\n",
    "    if 'rolling_cols' in locals():\n",
    "        feature_cols += rolling_cols\n",
    "    if 'derived_cols' in locals():\n",
    "        feature_cols += derived_cols\n",
    "    if 'advanced_cols' in locals():\n",
    "        feature_cols += advanced_cols\n",
    "    if 'structural_cols' in locals():\n",
    "        feature_cols += structural_cols\n",
    "    if 'lag_cols' in locals():\n",
    "        feature_cols += lag_cols\n",
    "    if 'promo_cols' in locals():\n",
    "        feature_cols += promo_cols\n",
    "    if 'holiday_cols' in locals():\n",
    "        feature_cols += holiday_cols\n",
    "    if 'event_cols' in locals():\n",
    "        feature_cols += event_cols\n",
    "    if 'oil_cols' in locals():\n",
    "        feature_cols += oil_cols\n",
    "\n",
    "    seen = set()\n",
    "    feature_cols = [c for c in feature_cols if not (c in seen or seen.add(c))]\n",
    "\n",
    "print(f\"âœ… Using {len(feature_cols)} final features for modelling\\n\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Basic breakdown summary (for logging / sanity)\n",
    "# ------------------------------------------------------------\n",
    "time_count        = len(time_features) if 'time_features' in locals() else 0\n",
    "cat_count         = len(categorical_features) if 'categorical_features' in locals() else 0\n",
    "rolling_count     = len(rolling_cols) if 'rolling_cols' in locals() else 0\n",
    "derived_count     = len(derived_cols) if 'derived_cols' in locals() else 0\n",
    "advanced_count    = len(advanced_cols) if 'advanced_cols' in locals() else 0\n",
    "structural_count  = len(structural_cols) if 'structural_cols' in locals() else 0\n",
    "lag_count         = len(lag_cols) if 'lag_cols' in locals() else 0\n",
    "promo_count       = len(promo_cols) if 'promo_cols' in locals() else 0\n",
    "holiday_count     = len(holiday_cols) if 'holiday_cols' in locals() else 0\n",
    "event_count       = len(event_cols) if 'event_cols' in locals() else 0\n",
    "oil_count         = len(oil_cols) if 'oil_cols' in locals() else 0\n",
    "\n",
    "hpcs_feature_count = rolling_count + derived_count + advanced_count + structural_count + lag_count\n",
    "domain_feature_count = promo_count + holiday_count + event_count + oil_count\n",
    "\n",
    "print(\" Feature Breakdown:\")\n",
    "print(\"============================================================\")\n",
    "print(f\"{'Category':<35} {'Count':>5}\")\n",
    "print(\"============================================================\")\n",
    "print(f\"{'Time features':<35} {time_count:>5}\")\n",
    "print(f\"{'Categorical features':<35} {cat_count:>5}\")\n",
    "base_other = len(['store_nbr', 'onpromotion', 'dcoilwtico', 'transactions'])\n",
    "print(f\"{'Other base features':<35} {base_other:>5}\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(f\"{'HPCSeries Core Features:':<35} {hpcs_feature_count:>5}\")\n",
    "print(f\"{'  - Rolling stats (7/28/56)':<35} {rolling_count:>5}\")\n",
    "print(f\"{'  - Derived ratios':<35} {derived_count:>5}\")\n",
    "print(f\"{'  - Robust anomaly detection':<35} {advanced_count:>5}\")\n",
    "print(f\"{'  - Structural (seasonality, CoV)':<35} {structural_count:>5}\")\n",
    "print(f\"{'  - Lag features':<35} {lag_count:>5}\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(f\"{'Domain-Specific Features:':<35} {domain_feature_count:>5}\")\n",
    "print(f\"{'  - Promotions':<35} {promo_count:>5}\")\n",
    "print(f\"{'  - Holidays':<35} {holiday_count:>5}\")\n",
    "print(f\"{'  - Events (payday, earthquake)':<35} {event_count:>5}\")\n",
    "print(f\"{'  - Oil prices':<35} {oil_count:>5}\")\n",
    "print(\"============================================================\")\n",
    "print(f\"{'TOTAL FEATURES':<35} {len(feature_cols):>5}\")\n",
    "print(\"============================================================\\n\")\n",
    "\n",
    "print(\"Ready for modelling: use `feature_cols` as your X columns\")\n",
    "\n",
    "FINAL_FEATURE_COLS = feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ train (Feature Engineering) memory usage: 0.743 GB (shape=(3000888, 97))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.7434213664382696)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mem_gb(train, \"train (Feature Engineering)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Fix robust_zscore_global in the full train DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing non-finite values in robust_zscore_global...\n",
      "============================================================\n",
      "Non-finite values in robust_zscore_global: 165,729\n",
      "  âœ“ Replaced 165,729 non-finite values with median=0.063899\n",
      "  âœ“ Cast robust_zscore_global to float32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Fixing non-finite values in robust_zscore_global...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "col = 'robust_zscore_global'\n",
    "\n",
    "if col in train.columns:\n",
    "    s = train[col]\n",
    "    arr = s.to_numpy()\n",
    "\n",
    "    mask_bad = ~np.isfinite(arr)\n",
    "    n_bad = int(mask_bad.sum())\n",
    "\n",
    "    print(f\"Non-finite values in {col}: {n_bad:,}\")\n",
    "\n",
    "    if n_bad > 0:\n",
    "        # Use median of finite values as replacement\n",
    "        finite_vals = s[np.isfinite(arr)]\n",
    "        if len(finite_vals) > 0:\n",
    "            replacement = finite_vals.median()\n",
    "        else:\n",
    "            replacement = 0.0 \n",
    "\n",
    "        train.loc[mask_bad, col] = replacement\n",
    "        print(f\"  âœ“ Replaced {n_bad:,} non-finite values with median={replacement:.6f}\")\n",
    "\n",
    "    train[col] = train[col].astype('float32')\n",
    "    print(f\"  âœ“ Cast {col} to float32\")\n",
    "else:\n",
    "    print(f\"âš ï¸ Column {col} not found in train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Train/Validation Split\n",
    "\n",
    "Split data chronologically (last 28 days as validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rebuilding train/validation split for Model v2...\n",
      "============================================================\n",
      "Train rows: 2,950,992, Val rows: 49,896\n",
      "Train dates: 2013-01-01 â†’ 2017-07-18\n",
      "Val dates:   2017-07-19 â†’ 2017-08-15\n",
      "\n",
      "Log targets created:\n",
      "  y_train_log range: [0.0000, 11.7338]\n",
      "  y_val_log   range: [0.0000, 9.8169]\n"
     ]
    }
   ],
   "source": [
    "print(\"Rebuilding train/validation split for Model v2...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "max_date = train['date'].max()\n",
    "val_cutoff = max_date - pd.Timedelta(days=VAL_HORIZON_DAYS)\n",
    "\n",
    "train_mask = train['date'] <= val_cutoff\n",
    "val_mask   = train['date'] >  val_cutoff\n",
    "\n",
    "train_df = train.loc[train_mask]\n",
    "val_df   = train.loc[val_mask]\n",
    "\n",
    "X_train = train_df[FINAL_FEATURE_COLS]\n",
    "y_train = train_df['sales'].astype('float32')\n",
    "X_val   = val_df[FINAL_FEATURE_COLS]\n",
    "y_val   = val_df['sales'].astype('float32')\n",
    "\n",
    "print(f\"Train rows: {X_train.shape[0]:,}, Val rows: {X_val.shape[0]:,}\")\n",
    "print(f\"Train dates: {train_df['date'].min().date()} â†’ {train_df['date'].max().date()}\")\n",
    "print(f\"Val dates:   {val_df['date'].min().date()} â†’ {val_df['date'].max().date()}\")\n",
    "\n",
    "# 2) Build log targets\n",
    "y_train_log = np.log1p(y_train.values)\n",
    "y_val_log   = np.log1p(y_val.values)\n",
    "\n",
    "print(\"\\nLog targets created:\")\n",
    "print(f\"  y_train_log range: [{y_train_log.min():.4f}, {y_train_log.max():.4f}]\")\n",
    "print(f\"  y_val_log   range: [{y_val_log.min():.4f}, {y_val_log.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ train (Split and Train) memory usage: 0.743 GB (shape=(3000888, 97))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.7434213664382696)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mem_gb(train, \"train (Split and Train)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup DMatrices & big leftovers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up v1 DMatrices and split data before Model v2...\n",
      "  - Deleted train_df\n",
      "  - Deleted val_df\n",
      "GC complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "print(\"Cleaning up v1 DMatrices and split data before Model v2...\")\n",
    "\n",
    "# Delete v1 DMatrices if they exist\n",
    "for name in [\"dtrain\", \"dval\"]:\n",
    "    if name in globals():\n",
    "        del globals()[name]\n",
    "        print(f\"  - Deleted {name}\")\n",
    "                             \n",
    "for name in [\"train_df\", \"val_df\"]:\n",
    "    if name in globals():\n",
    "        del globals()[name]\n",
    "        print(f\"  - Deleted {name}\")\n",
    "\n",
    "gc.collect()\n",
    "print(\"GC complete.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Train XGBoost Model\n",
    "\n",
    "Train gradient boosting model with CPU-optimized parameters.\n",
    "\n",
    "**Parameters tuned for AMD Ryzen 4C/8T with AVX2:**\n",
    "- `tree_method='hist'` - Fast histogram-based algorithm\n",
    "- `nthread=8` - Use all 8 threads\n",
    "- Early stopping to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model v2 â€“ training XGBoost with log1p(sales) target...\n",
      "============================================================\n",
      "CPU threads available: 8\n",
      "X_train: (2950992, 87), X_val: (49896, 87)\n",
      "\n",
      " Freed pandas feature matrices; training using DMatrix only.\n",
      "\n",
      "Parameters (Model v2 â€“ log target):\n",
      "  objective            = reg:squarederror\n",
      "  eval_metric          = rmse\n",
      "  max_depth            = 5\n",
      "  eta                  = 0.1\n",
      "  subsample            = 0.7\n",
      "  colsample_bytree     = 0.7\n",
      "  min_child_weight     = 3\n",
      "  lambda               = 1.0\n",
      "  alpha                = 0.0\n",
      "  tree_method          = hist\n",
      "  max_bin              = 128\n",
      "  nthread              = 8\n",
      "  seed                 = 42\n",
      "  verbosity            = 1\n",
      "\n",
      "Starting training (Model v2)...\n",
      "------------------------------------------------------------\n",
      "[0]\ttrain-rmse:2.43569\tval-rmse:2.36942\n",
      "[50]\ttrain-rmse:0.20962\tval-rmse:0.17058\n",
      "[100]\ttrain-rmse:0.16616\tval-rmse:0.14064\n",
      "[150]\ttrain-rmse:0.14584\tval-rmse:0.12287\n",
      "[200]\ttrain-rmse:0.13250\tval-rmse:0.11132\n",
      "[250]\ttrain-rmse:0.12406\tval-rmse:0.10415\n",
      "[300]\ttrain-rmse:0.11639\tval-rmse:0.09805\n",
      "[350]\ttrain-rmse:0.11079\tval-rmse:0.09356\n",
      "[400]\ttrain-rmse:0.10629\tval-rmse:0.09033\n",
      "[450]\ttrain-rmse:0.10261\tval-rmse:0.08739\n",
      "[499]\ttrain-rmse:0.09945\tval-rmse:0.08514\n",
      "------------------------------------------------------------\n",
      "\n",
      "âœ“ Model v2 training complete in 131.0 seconds (2.18 minutes)\n",
      "  Best iteration (log-space): 499\n",
      "  Best log-RMSE:             0.085138\n",
      "\n",
      " Model v2 saved to models/hpcs_xgb_store_sales_log.json\n"
     ]
    }
   ],
   "source": [
    "print(\"Model v2 â€“ training XGBoost with log1p(sales) target...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if not XGB_AVAILABLE:\n",
    "    print(\"âŒ XGBoost not available. Skipping.\")\n",
    "else:\n",
    "    import multiprocessing, gc \n",
    "\n",
    "    n_threads = multiprocessing.cpu_count()\n",
    "    print(f\"CPU threads available: {n_threads}\")\n",
    "    print(f\"X_train: {X_train.shape}, X_val: {X_val.shape}\\n\")\n",
    "\n",
    "    # Reuse v1 params but keep them separate\n",
    "    params_v2 = dict(params_v1)\n",
    "    params_v2.setdefault(\"objective\", \"reg:squarederror\")\n",
    "    params_v2.setdefault(\"eval_metric\", \"rmse\")\n",
    "    params_v2[\"nthread\"] = n_threads \n",
    "\n",
    "    # Build DMatrices\n",
    "    dtrain_log = xgb.DMatrix(X_train, label=y_train_log, missing=np.nan)\n",
    "    dval_log   = xgb.DMatrix(X_val,   label=y_val_log,   missing=np.nan)\n",
    "\n",
    "    del X_train, X_val, y_train_log, y_val_log\n",
    "    try:\n",
    "        del train_df, val_df \n",
    "    except NameError:\n",
    "        pass\n",
    "    \n",
    "    gc.collect()\n",
    "    print(\" Freed pandas feature matrices; training using DMatrix only.\\n\")\n",
    "\n",
    "\n",
    "    print(\"Parameters (Model v2 â€“ log target):\")\n",
    "    for k, v in params_v2.items():\n",
    "        print(f\"  {k:20s} = {v}\")\n",
    "\n",
    "    evals_v2 = [(dtrain_log, \"train\"), (dval_log, \"val\")]\n",
    "\n",
    "    print(\"\\nStarting training (Model v2)...\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    start_t = time.time()\n",
    "    model_log = xgb.train(\n",
    "        params=params_v2,\n",
    "        dtrain=dtrain_log,\n",
    "        num_boost_round=500,\n",
    "        evals=evals_v2,\n",
    "        early_stopping_rounds=20,\n",
    "        verbose_eval=50,\n",
    "    )\n",
    "    train_time_v2 = time.time() - start_t\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"\\nâœ“ Model v2 training complete in {train_time_v2:.1f} seconds ({train_time_v2/60:.2f} minutes)\")\n",
    "    print(f\"  Best iteration (log-space): {model_log.best_iteration}\")\n",
    "    print(f\"  Best log-RMSE:             {model_log.best_score:.6f}\")\n",
    "\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    model_v2_path = \"models/hpcs_xgb_store_sales_log.json\"\n",
    "    model_log.save_model(model_v2_path)\n",
    "    print(f\"\\n Model v2 saved to {model_v2_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model v2 in raw space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model v2 â€“ evaluating with RMSLE...\n",
      "============================================================\n",
      "âœ… Bound model_log â†’ model (Model v2: log1p(sales) target)\n",
      "Validation rows: 49,896\n",
      "Validation feature shape: (49896, 87)\n",
      "\n",
      "\n",
      " Model v2 Validation Metrics\n",
      "   (Primary: RMSLE â€“ Kaggle competition metric)\n",
      "------------------------------------------------------------\n",
      "  RMSLE (log-space):                0.085138\n",
      "  RMSE  (raw sales):                 86.6120\n",
      "  MAE   (raw sales):                 23.1533\n",
      "  MAPE  (raw sales):                    5.20%\n",
      "------------------------------------------------------------\n",
      "        RMSE/MAE/MAPE are shown only for interpretation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Model v2 â€“ evaluating with RMSLE...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 0) Bind the trained Model v2 into a generic `model` var\n",
    "# --------------------------------------------------------\n",
    "if 'model_log' not in globals():\n",
    "    raise RuntimeError(\"model_log is not defined. Train Model v2 before running this cell.\")\n",
    "\n",
    "model = model_log\n",
    "print(\"âœ… Bound model_log â†’ model (Model v2: log1p(sales) target)\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 1) Rebuild validation slice from `train`\n",
    "#    (last 28 days = validation)\n",
    "# --------------------------------------------------------\n",
    "val_horizon_days = 28\n",
    "max_date = train['date'].max()\n",
    "val_cutoff = max_date - pd.Timedelta(days=val_horizon_days)\n",
    "\n",
    "val_df = train[train['date'] > val_cutoff].copy()\n",
    "\n",
    "FEATURES_FOR_MODEL = (\n",
    "    FINAL_FEATURE_COLS\n",
    "    if 'FINAL_FEATURE_COLS' in globals()\n",
    "    else feature_cols\n",
    ")\n",
    "\n",
    "X_val_eval = val_df[FEATURES_FOR_MODEL].astype('float32')\n",
    "y_val_eval = val_df['sales'].astype('float32').values\n",
    "\n",
    "print(f\"Validation rows: {X_val_eval.shape[0]:,}\")\n",
    "print(f\"Validation feature shape: {X_val_eval.shape}\\n\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 2) Build DMatrix and predict in LOG space\n",
    "# --------------------------------------------------------\n",
    "dval_eval = xgb.DMatrix(X_val_eval, missing=np.nan)\n",
    "\n",
    "best_ntree_limit_v2 = getattr(model, \"best_ntree_limit\", None)\n",
    "if best_ntree_limit_v2 is not None and best_ntree_limit_v2 > 0:\n",
    "    val_pred_log = model.predict(dval_eval, ntree_limit=best_ntree_limit_v2)\n",
    "else:\n",
    "    val_pred_log = model.predict(dval_eval)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 3) Compute RMSLE (competition metric) in log space\n",
    "# --------------------------------------------------------\n",
    "y_val_log = np.log1p(y_val_eval)          # log1p(true)\n",
    "rmsle_v2 = np.sqrt(np.mean((y_val_log - val_pred_log) ** 2))\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 4) Also compute RAW-space metrics for intuition\n",
    "# --------------------------------------------------------\n",
    "val_preds_raw = np.expm1(val_pred_log)\n",
    "val_preds_raw = np.maximum(0, val_preds_raw)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "rmse_v2 = np.sqrt(mean_squared_error(y_val_eval, val_preds_raw))\n",
    "mae_v2  = mean_absolute_error(y_val_eval, val_preds_raw)\n",
    "mape_v2 = 100 * np.mean(np.abs((y_val_eval - val_preds_raw) / (y_val_eval + 1)))\n",
    "\n",
    "print(\"\\n Model v2 Validation Metrics\")\n",
    "print(\"   (Primary: RMSLE â€“ Kaggle competition metric)\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"  RMSLE (log-space):              {rmsle_v2:>10.6f}\")\n",
    "print(f\"  RMSE  (raw sales):              {rmse_v2:>10.4f}\")\n",
    "print(f\"  MAE   (raw sales):              {mae_v2:>10.4f}\")\n",
    "print(f\"  MAPE  (raw sales):              {mape_v2:>10.2f}%\")\n",
    "print(\"-\" * 60)\n",
    "print(\"        RMSE/MAE/MAPE are shown only for interpretation.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper: build feature matrix for any DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 3.1 â€“ Helper: build feature matrix for inference\n",
      "========================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Section 3.1 â€“ Helper: build feature matrix for inference\")\n",
    "print(\"========================================================\")\n",
    "\n",
    "def build_feature_matrix(df, feature_list):\n",
    "    \"\"\"\n",
    "    Given a DataFrame `df` and a list of feature names `feature_list`,\n",
    "    return a float32 feature matrix suitable for XGBoost (DMatrix).\n",
    "    \n",
    "    - Only keeps columns that actually exist in df.\n",
    "    - Raises a warning if some expected features are missing.\n",
    "    \"\"\"\n",
    "    # Ensure we use the final agreed feature set\n",
    "    features = list(feature_list)\n",
    "\n",
    "    # Check which features are available in this df\n",
    "    missing = [c for c in features if c not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"âš ï¸ Warning: {len(missing)} expected features are missing in this DataFrame.\")\n",
    "        print(\"   Examples:\", missing[:10])\n",
    "        print(\"   (Make sure your test set went through the same feature engineering pipeline.)\")\n",
    "\n",
    "    existing = [c for c in features if c in df.columns]\n",
    "    if not existing:\n",
    "        raise ValueError(\"No overlapping features between df and feature_list.\")\n",
    "\n",
    "    X = df[existing].astype('float32')\n",
    "    print(f\"âœ… Built feature matrix with shape {X.shape} (float32)\")\n",
    "    return X, existing  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Predict helper using Model v2 (log1p(sales) target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Section 3.2 â€“ Helper: predict with Model v2 (log1p(sales))\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSection 3.2 â€“ Helper: predict with Model v2 (log1p(sales))\")\n",
    "print(\"============================================================\")\n",
    "\n",
    "def predict_with_model_v2(df, feature_list, model_obj=None):\n",
    "    \"\"\"\n",
    "    Run predictions on a DataFrame `df` using Model v2 (log1p(sales) target).\n",
    "\n",
    "    Returns:\n",
    "        preds_raw: np.ndarray of predictions in RAW sales space.\n",
    "        used_features: list of feature names actually used (column order).\n",
    "    \"\"\"\n",
    "    if model_obj is None:\n",
    "        if 'model_log' not in globals():\n",
    "            raise RuntimeError(\"Model v2 (model_log) is not available in this notebook.\")\n",
    "        model_obj = model_log\n",
    "\n",
    "    # Build feature matrix\n",
    "    X, used_features = build_feature_matrix(df, feature_list)\n",
    "\n",
    "    # Build DMatrix\n",
    "    dmat = xgb.DMatrix(X, missing=np.nan)\n",
    "    del X\n",
    "\n",
    "    # Use best_ntree_limit if early stopping was used\n",
    "    best_ntree_limit = getattr(model_obj, \"best_ntree_limit\", None)\n",
    "    if best_ntree_limit is not None and best_ntree_limit > 0:\n",
    "        preds_log = model_obj.predict(dmat, ntree_limit=best_ntree_limit)\n",
    "    else:\n",
    "        preds_log = model_obj.predict(dmat)\n",
    "\n",
    "    # Back-transform to raw sales\n",
    "    preds_raw = np.expm1(preds_log)\n",
    "    preds_raw = np.maximum(0, preds_raw)\n",
    "\n",
    "    print(f\"âœ… Generated {len(preds_raw):,} predictions (raw sales space)\")\n",
    "    return preds_raw, used_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure all model features exist in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syncing feature columns between train and test...\n",
      "================================================\n",
      "Adding 72 missing feature columns to test as NaN.\n",
      "âœ“ Feature column sync complete.\n",
      "Total columns in test: 95\n"
     ]
    }
   ],
   "source": [
    "print(\"Syncing feature columns between train and test...\")\n",
    "print(\"================================================\")\n",
    "\n",
    "# Decide which feature list to use (same as for training)\n",
    "FEATURES_FOR_MODEL = (\n",
    "    FINAL_FEATURE_COLS\n",
    "    if 'FINAL_FEATURE_COLS' in globals()\n",
    "    else feature_cols\n",
    ")\n",
    "\n",
    "missing_in_test = [c for c in FEATURES_FOR_MODEL if c not in test.columns]\n",
    "if missing_in_test:\n",
    "    print(f\"Adding {len(missing_in_test)} missing feature columns to test as NaN.\")\n",
    "    for c in missing_in_test:\n",
    "        test[c] = np.nan\n",
    "else:\n",
    "    print(\"All model features already present in test.\")\n",
    "\n",
    "print(\"âœ“ Feature column sync complete.\")\n",
    "print(f\"Total columns in test: {len(test.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 â€œTest-likeâ€ prediction run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Section 3.3 â€“ Prepare test-style features & run Model v2 predictions\n",
      "====================================================================\n",
      "Using `test` DataFrame for prediction\n",
      "Using 87 features for inference.\n",
      "âœ… Built feature matrix with shape (28512, 87) (float32)\n",
      "âœ… Generated 28,512 predictions (raw sales space)\n",
      "\n",
      "âœ… Attached predictions to `test` as column `pred_sales_v2_raw`.\n",
      "        date  store_nbr      family  pred_sales_v2_raw\n",
      "0 2017-08-16          1  AUTOMOTIVE      115750.359375\n",
      "1 2017-08-16          1   BABY CARE      115750.359375\n",
      "2 2017-08-16          1      BEAUTY      110794.062500\n",
      "3 2017-08-16          1   BEVERAGES      111484.171875\n",
      "4 2017-08-16          1       BOOKS      115750.359375\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSection 3.3 â€“ Prepare test-style features & run Model v2 predictions\")\n",
    "print(\"====================================================================\")\n",
    "\n",
    "# Decide which DataFrame to treat as \"test\" for now\n",
    "if 'test' in globals():\n",
    "    target_df_name = 'test'\n",
    "    target_df = test\n",
    "    print(\"Using `test` DataFrame for prediction\")\n",
    "else:\n",
    "    target_df_name = 'val_df'\n",
    "    if 'val_df' not in globals():\n",
    "        print(\"`test` not found; rebuilding validation slice as proxy test set...\")\n",
    "        val_horizon_days = 28\n",
    "        max_date = train['date'].max()\n",
    "        val_cutoff = max_date - pd.Timedelta(days=val_horizon_days)\n",
    "        val_df = train[train['date'] > val_cutoff].copy()\n",
    "\n",
    "    target_df = val_df\n",
    "    print(\"âš ï¸ `test` DataFrame not found. Using `val_df` as a test proxy for now.\")\n",
    "\n",
    "# Decide which feature list to use\n",
    "FEATURES_FOR_MODEL = (\n",
    "    FINAL_FEATURE_COLS\n",
    "    if 'FINAL_FEATURE_COLS' in globals()\n",
    "    else feature_cols\n",
    ")\n",
    "\n",
    "print(f\"Using {len(FEATURES_FOR_MODEL)} features for inference.\")\n",
    "\n",
    "# Run predictions with Model v2\n",
    "test_like_preds, used_features = predict_with_model_v2(\n",
    "    df=target_df,\n",
    "    feature_list=FEATURES_FOR_MODEL,\n",
    "    model_obj=model_log,\n",
    ")\n",
    "\n",
    "# Attach predictions back to the DataFrame for inspection\n",
    "pred_col_name = \"pred_sales_v2_raw\"\n",
    "target_df[pred_col_name] = test_like_preds\n",
    "\n",
    "print(f\"\\nâœ… Attached predictions to `{target_df_name}` as column `{pred_col_name}`.\")\n",
    "print(target_df[[ 'date', 'store_nbr', 'family', pred_col_name ]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 â€“ Generate predictions from test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 4.1 â€“ Build Kaggle-style submission DataFrame (Model v2)\n",
      "================================================================\n",
      "Using 87 features for submission prediction.\n",
      "âœ… Built feature matrix with shape (28512, 87) (float32)\n",
      "âœ… Generated 28,512 predictions (raw sales space)\n",
      "\n",
      "âœ… Built `submission_v2_df` with shape: (28512, 2)\n",
      "        id          sales\n",
      "0  3000888  115750.359375\n",
      "1  3000889  115750.359375\n",
      "2  3000890  110794.062500\n",
      "3  3000891  111484.171875\n",
      "4  3000892  115750.359375\n"
     ]
    }
   ],
   "source": [
    "print(\"Section 4.1 â€“ Build Kaggle-style submission DataFrame (Model v2)\")\n",
    "print(\"================================================================\")\n",
    "\n",
    "# 1) Sanity checks\n",
    "if not XGB_AVAILABLE:\n",
    "    raise RuntimeError(\"XGBoost is not available. Load xgboost before running Section 4.\")\n",
    "\n",
    "if 'model_log' not in globals():\n",
    "    raise RuntimeError(\"Model v2 (model_log) not found. Train Model v2 before Section 4.\")\n",
    "\n",
    "if 'test' not in globals():\n",
    "    raise RuntimeError(\"`test` DataFrame not found. Load `test.csv` and apply the same feature engineering pipeline as `train`.\")\n",
    "\n",
    "if 'id' not in test.columns:\n",
    "    raise RuntimeError(\"`test` DataFrame has no 'id' column. Kaggle needs 'id' + 'sales' in the submission.\")\n",
    "\n",
    "# 2) Decide which feature list to use\n",
    "FEATURES_FOR_MODEL = (\n",
    "    FINAL_FEATURE_COLS\n",
    "    if 'FINAL_FEATURE_COLS' in globals()\n",
    "    else feature_cols\n",
    ")\n",
    "\n",
    "print(f\"Using {len(FEATURES_FOR_MODEL)} features for submission prediction.\")\n",
    "\n",
    "# 3) Run Model v2 predictions on test\n",
    "test_preds_v2, used_features_test = predict_with_model_v2(\n",
    "    df=test,\n",
    "    feature_list=FEATURES_FOR_MODEL,\n",
    "    model_obj=model_log,\n",
    ")\n",
    "\n",
    "# 4) Build submission DataFrame\n",
    "submission_v2_df = pd.DataFrame({\n",
    "    \"id\": test[\"id\"].astype(\"int64\"),\n",
    "    \"sales\": test_preds_v2.astype(\"float32\"),\n",
    "})\n",
    "\n",
    "print(\"\\nâœ… Built `submission_v2_df` with shape:\", submission_v2_df.shape)\n",
    "print(submission_v2_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 â€“ Save submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Kaggle submission CSV for Model v2 (log1p(sales) target)...\n",
      "============================================================\n",
      "âœ“ submission_v2_df has 28512 rows and required columns ['id', 'sales']\n",
      "\n",
      "Preview of submission_out:\n",
      "        id          sales\n",
      "0  3000888  115750.359375\n",
      "1  3000889  115750.359375\n",
      "2  3000890  110794.062500\n",
      "3  3000891  111484.171875\n",
      "4  3000892  115750.359375\n",
      "\n",
      "âœ… Saved Kaggle submission CSV to: submissions/hpcs_kaggle_store_sales_v2_log.csv\n",
      "   Format: id,sales with 28,512 rows (plus header)\n",
      "ðŸ’¾ Metadata saved to: submissions/hpcs_kaggle_store_sales_v2_log_metadata.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "print(\"Saving Kaggle submission CSV for Model v2 (log1p(sales) target)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Basic validation of submission_v2_df\n",
    "# ------------------------------------------------------------------\n",
    "if 'submission_v2_df' not in globals():\n",
    "    raise RuntimeError(\"submission_v2_df not found. Run the prediction step (Section 4.2) first.\")\n",
    "\n",
    "# Ensure correct columns and row count\n",
    "expected_rows = 28512\n",
    "required_cols = ['id', 'sales']\n",
    "\n",
    "missing_cols = [c for c in required_cols if c not in submission_v2_df.columns]\n",
    "if missing_cols:\n",
    "    raise RuntimeError(f\"submission_v2_df is missing required columns: {missing_cols}\")\n",
    "\n",
    "if submission_v2_df.shape[0] != expected_rows:\n",
    "    raise RuntimeError(\n",
    "        f\"submission_v2_df has {submission_v2_df.shape[0]} rows, \"\n",
    "        f\"but Kaggle expects {expected_rows}.\"\n",
    "    )\n",
    "\n",
    "print(f\"âœ“ submission_v2_df has {submission_v2_df.shape[0]} rows and required columns {required_cols}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Build final Kaggle submission frame (id,sales)\n",
    "# ------------------------------------------------------------------\n",
    "submission_out = submission_v2_df[required_cols].copy()\n",
    "\n",
    "# Sort by id just to be neat (not strictly required by Kaggle)\n",
    "submission_out = submission_out.sort_values('id').reset_index(drop=True)\n",
    "\n",
    "# Ensure numeric and non-negative sales\n",
    "submission_out['sales'] = submission_out['sales'].astype('float32')\n",
    "submission_out['sales'] = submission_out['sales'].clip(lower=0)\n",
    "\n",
    "print(\"\\nPreview of submission_out:\")\n",
    "print(submission_out.head())\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Save to CSV in a dedicated folder\n",
    "# ------------------------------------------------------------------\n",
    "os.makedirs(\"submissions\", exist_ok=True)\n",
    "\n",
    "submission_path = \"submissions/hpcs_kaggle_store_sales_v2_log.csv\"\n",
    "submission_out.to_csv(submission_path, index=False, float_format=\"%.4f\")\n",
    "\n",
    "print(f\"\\nâœ… Saved Kaggle submission CSV to: {submission_path}\")\n",
    "print(\"   Format: id,sales with 28,512 rows (plus header)\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) (Optional) Save run metadata for traceability\n",
    "# ------------------------------------------------------------------\n",
    "metadata = {\n",
    "    \"model_name\": \"HPCSeries_XGBoost_StoreSales_Model_v2_log\",\n",
    "    \"description\": \"XGBoost with log1p(sales) target + HPCSeries features\",\n",
    "    \"features\": (\n",
    "        FINAL_FEATURE_COLS\n",
    "        if 'FINAL_FEATURE_COLS' in globals()\n",
    "        else feature_cols\n",
    "    ),\n",
    "    \"metrics\": {\n",
    "        # Fill from your earlier cells if you kept them in variables\n",
    "        \"v2_rmsle\":  float(0.085138),   # competition metric (from your log-RMSE)\n",
    "        \"v2_rmse\":   float(86.6120),\n",
    "        \"v2_mae\":    float(23.1533),\n",
    "        \"v2_mape\":   float(5.20),\n",
    "        # Optionally, you can add v1 metrics here too if stored\n",
    "        # \"v1_rmse\": float(...),\n",
    "        # \"v1_mae\":  float(...),\n",
    "        # \"v1_mape\": float(...),\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"num_train_rows\": int(2950992),\n",
    "        \"num_val_rows\": int(49896),\n",
    "        \"num_features\": int(len(\n",
    "            FINAL_FEATURE_COLS\n",
    "            if 'FINAL_FEATURE_COLS' in globals()\n",
    "            else feature_cols\n",
    "        )),\n",
    "        \"params\": {\n",
    "            \"objective\":        \"reg:squarederror\",\n",
    "            \"eval_metric\":      \"rmse\",\n",
    "            \"max_depth\":        5,\n",
    "            \"eta\":              0.1,\n",
    "            \"subsample\":        0.7,\n",
    "            \"colsample_bytree\": 0.7,\n",
    "            \"min_child_weight\": 3,\n",
    "            \"lambda\":           1.0,\n",
    "            \"alpha\":            0.0,\n",
    "            \"tree_method\":      \"hist\",\n",
    "            \"max_bin\":          128,\n",
    "            \"nthread\":          8,\n",
    "            \"seed\":             42,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "metadata_path = \"submissions/hpcs_kaggle_store_sales_v2_log_metadata.json\"\n",
    "with open(metadata_path, \"w\") as f:\n",
    "    json.dump(metadata, f, indent=4)\n",
    "\n",
    "print(f\" Metadata saved to: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Results \n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "âœ… **Data Loading & Prep**\n",
    "- Loaded and joined **3M+ rows** of Store Sales data (train) with stores, oil, holidays, transactions.\n",
    "- Engineered **87 final features** per row (time, HPCSeries, domain-specific).\n",
    "\n",
    "âœ… **HPCSeries Feature Blocks**\n",
    "- **Rolling stats (Block A):**\n",
    "  - Windows: 7, 28, 56 days\n",
    "  - Stats: mean, median, std, MAD\n",
    "- **Derived ratios (Block A):**\n",
    "  - MAD / median (robust CoV)\n",
    "  - STD / MAD (outlier index)\n",
    "  - MAD / mean (volatility index)\n",
    "- **Advanced robust stats (Block B):**\n",
    "  - Global robust z-scores (MAD-based)\n",
    "  - Clipped z-scores, anomaly flags, anomaly density\n",
    "  - Group aggregates: mean, std, median, MAD, std-based anomaly score\n",
    "- **Structural features (Block C):**\n",
    "  - Day-of-week seasonality strength\n",
    "  - Coefficient of variation of sales\n",
    "- **Lag features (Block A extension):**\n",
    "  - sales_lag_7, sales_lag_14, sales_lag_28\n",
    "\n",
    "Total **HPCSeries-driven features:** **35**  \n",
    "Total **domain-specific features (promo, holidays, events, oil):** **34**  \n",
    "\n",
    "âœ… **Model Training (XGBoost)**\n",
    "- Tree method: `hist` (CPU-optimized)\n",
    "- Objective: `reg:squarederror`\n",
    "- 87 features, ~2.95M training rows\n",
    "- **Training time:** ~107 seconds (AMD Ryzen 4C/8T)\n",
    "- **Early stopping:** best iteration â‰ˆ 399\n",
    "\n",
    "âœ… **Validation Performance (last 28 days)**\n",
    "- **RMSE:** â‰ˆ **62.00**\n",
    "- **MAE:** â‰ˆ **21.04**\n",
    "- **MAPE:** â‰ˆ **24.0%**\n",
    "- Non-negative predictions enforced.\n",
    "\n",
    "### Feature Importance Highlights\n",
    "\n",
    "Top drivers (by gain):\n",
    "\n",
    "- **Rolling 7-day stats:** `sales_roll_mean_7`, `sales_roll_median_7`\n",
    "- **Longer windows:** `sales_roll_mean_28`, `sales_roll_mean_56`\n",
    "- **Store activity:** `transactions`\n",
    "- **Robust anomalies:** `robust_zscore_global`, `robust_zscore_clipped`, `is_anom_robust_3`\n",
    "- **Group baselines:** `sales_group_median`, `sales_group_mean`, `sales_group_std`\n",
    "- **Calendar & holidays:** `dayofweek`, `quarter`, `is_quarter_start`, `holiday_lag_1`\n",
    "- **Seasonality:** `sf_dow_seasonality_strength`\n",
    "\n",
    "This confirms that **HPCSeries rolling + robust features** are central to model performance.\n",
    "\n",
    "### HPCSeries Performance Benefits (Qualitative)\n",
    "\n",
    "- SIMD-accelerated (AVX2) rolling & robust statistics on 3M+ rows.\n",
    "- Per-group processing (store, family) for cache-friendly, parallelizable loops.\n",
    "- MAD-based robust z-scores and volatility measures to handle outliers.\n",
    "- Clean separation between:\n",
    "  - Core numeric kernels (HPCSeries)\n",
    "  - Domain features (promos, holidays, events, oil)\n",
    "  - Model layer (XGBoost)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n",
    "\n",
    "print(\"Kaggle Submission Format:\")\n",
    "print(\"=\"*60)\n",
    "print(sample_submission.head(10))\n",
    "print(f\"\\nShape: {sample_submission.shape}\")\n",
    "print(f\"\\nColumns: {list(sample_submission.columns)}\")\n",
    "print(f\"\\nTo submit predictions:\")\n",
    "print(\"  1. Generate predictions for test set\")\n",
    "print(\"  2. Create DataFrame with 'id' and 'sales' columns\")\n",
    "print(\"  3. Save as: submission.to_csv('submission.csv', index=False)\")\n",
    "print(\"  4. Upload to Kaggle competition page\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
