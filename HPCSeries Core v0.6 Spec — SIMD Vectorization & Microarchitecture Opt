It looks like the **file-generation tool is temporarily unavailable**, so writing the `.md` file directly from Python isn’t possible right now.

**No problem — here is your full v0.6 spec as clean Markdown that you can copy/paste straight into your repo.**

You will get **the exact same content** you intended to download.

---

# ✅ **HPCSeries Core v0.6 Specification — SIMD Vectorization & Microarchitecture Optimization (CPU-Only)**

**Status:** Architect Draft
**Scope:** CPU SIMD kernels, microarchitecture-tuned code paths, *no ABI changes*
**Depends on:** v0.5 CPU Auto-Tuning Runtime

---

## **1. Overview**

HPCSeries Core v0.6 introduces **explicit SIMD vectorization** and **microarchitecture-specific optimizations** for major kernels.

v0.5 gave us:

* CPU feature detection
* NUMA awareness
* Benchmark-based tuning
* SIMD capability detection

v0.6 now implements:

* Real vectorized kernel paths (OpenMP SIMD + optional intrinsics)
* Runtime dispatch (SSE2 / AVX / AVX2 / AVX-512 / NEON)
* Memory alignment improvements
* Loop unrolling & cache-aware layouts
* Integration with v0.5 auto-tuning thresholds

This version focuses entirely on **maximizing performance per core**.

---

## **2. Goals**

### **Primary Goals**

* Add explicit SIMD vectorization
* Introduce SIMD runtime dispatch
* Use memory alignment for vector loads
* Tune kernels using SIMD width metadata
* Maintain compatibility with all earlier versions
* NO ABI changes

### **Non-Goals**

* No GPU kernels
* No Python bindings (v0.7)
* No new algorithms
* No masked-op SIMD (too branch-heavy)
* No 3D kernels

---

## **3. SIMD Targets for v0.6**

### SIMD-optimized kernels:

#### **Reductions**

* `reduce_sum`
* `reduce_mean`
* `reduce_min` / `reduce_max`
* `reduce_variance` / `reduce_std`

#### **Rolling**

* `rolling_mean`
* `rolling_variance`
* `rolling_std`

#### **Robust stats**

* `zscore`
* `robust_zscore`

#### **2D Axis**

* `reduce_sum_axis0`
* `reduce_mean_axis0`

### **Excluded (not SIMD-friendly)**

* rolling median / rolling MAD (tree/heap operations)
* masked rolling ops
* anomaly detection
* grouped reductions (non-contiguous access)

---

## **4. SIMD Implementation Strategy**

### **4.1 OpenMP SIMD (primary mechanism)**

Use compiler-guided vectorization:

```fortran
!$omp simd aligned(x, y:64)
do i = 1, n
    y(i) = x(i) * scale
end do
```

Benefits:

* portable
* readable
* compiler optimised
* automatically picks the widest ISA (AVX/AVX2/AVX-512/NEON)

---

### **4.2 Optional Intrinsics (advanced small subset)**

For reductions only, provide:

* SSE2
* AVX
* AVX2 (256-bit)
* AVX-512 (512-bit)
* NEON (ARM)

Pattern:

```
if simd = AVX512 → call reduce_sum_avx512
else if simd = AVX2 → call reduce_sum_avx2
else → use OpenMP SIMD
```

---

### **4.3 Memory Alignment**

Introduce internal alignment helpers:

* 32-byte alignment for SSE/AVX
* 64-byte alignment for AVX-512

Use tail-scalar loops for leftover elements.

---

## **5. Runtime SIMD Dispatch (uses v0.5 CPU detection)**

v0.5 provides SIMD metadata:

* SSE2
* AVX
* AVX2
* AVX-512
* NEON

v0.6 uses this to:

* pick the best SIMD kernel path
* determine chunk/block sizes
* modify parallel thresholds
* store optimized choices in config (`~/.hpcseries/config.json`)

This creates a CPU-optimized BLAS-style dispatch layer.

---

## **6. Microarchitecture Optimizations**

### **6.1 Cache-aware blocking**

Especially for large 2D axis reductions.

### **6.2 Prefetch-friendly loops**

Enhanced linear scanning.

### **6.3 Loop unrolling**

Using OpenMP or compiler flags.

### **6.4 False-sharing avoidance**

Use 64-byte padded accumulators for threads.

---

## **7. Affected Kernel Classes**

### **SIMD-enhanced in v0.6**

* core 1D reductions
* rolling mean/std
* zscore & robust zscore
* axis-0 reductions

### **Excluded**

* rolling median/MAD
* fast C++ rolling ops
* masked ops
* anomaly detection
* group reductions

---

## **8. New Internal Modules**

* `hpcs_simd_dispatch` — ISA routing logic
* `hpcs_reduce_simd` — SIMD reduction implementations
* `hpcs_simd_align` — alignment utilities

These do **not** affect the public API.

---

## **9. API and ABI Impact**

### **Public API**

* No new public functions
* No signature changes
* All kernels maintain:

```
void hpcs_xxx(..., int *status)
```

### **Internal API**

* New dispatch tables
* New SIMD modules

---

## **10. Testing Requirements**

### **Correctness**

* Compare SIMD vs scalar outputs
* Validate tail-loop precision
* Cross-architecture tests (Intel, AMD, ARM)

### **Performance**

* Measure scalar vs OpenMP vs SIMD-only vs SIMD+OpenMP
* Compare intrinsics vs OpenMP SIMD

### **Auto-tuning validation**

* Ensure SIMD width influences thresholds
* Validate persistent config performance

---

## **11. Out of Scope**

* GPU kernels
* Python API
* SIMD for masked ops
* New feature kernels
* Full fast-median SIMD rewrite

---

## **12. Summary**

v0.6 introduces:

* **true SIMD acceleration**
* **microarchitecture-sensitive optimizations**
* **runtime dispatch for ISA variants**
* **aligned memory and tuned loops**

This makes HPCSeries a **hybrid vectorized + multithreaded** engine and prepares the foundation for the v0.7 Python bindings and eventual v1.0 launch.

---

# ✅ Done

If you want, I can now generate:

* **v0.6 Completion Checklist**
* **v0.6 Implementation Roadmap (agent-mode)**
* **v0.7 Python Bindings Spec**

Just tell me!
