{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Optimization and Calibration (v0.7)\n",
    "\n",
    "This notebook demonstrates how to use HPCSeries Core's auto-tuning calibration system to optimize performance for your specific hardware.\n",
    "\n",
    "## What is Auto-Tuning Calibration?\n",
    "\n",
    "HPCSeries automatically determines optimal:\n",
    "- **Parallelization thresholds** (when to use multi-threading)\n",
    "- **Thread counts** for different operation types\n",
    "- **NUMA modes** for memory-intensive operations\n",
    "\n",
    "This ensures maximum performance on your specific CPU architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hpcs\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. System Information\n",
    "\n",
    "First, let's check your system configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display CPU information\n",
    "!hpcs cpuinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quick Calibration\n",
    "\n",
    "Quick calibration uses hardware heuristics (5-10 seconds):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run quick calibration\n",
    "print(\"Running quick calibration...\\n\")\n",
    "start = time.time()\n",
    "hpcs.calibrate(quick=True)\n",
    "elapsed = time.time() - start\n",
    "print(f\"\\nCalibration completed in {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save and Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save configuration\n",
    "config_path = os.path.expanduser(\"~/.hpcs/config.json\")\n",
    "hpcs.save_calibration_config(config_path)\n",
    "print(f\"Configuration saved to: {config_path}\")\n",
    "\n",
    "# Load and display configuration\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"\\nConfiguration contents:\")\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding Configuration Parameters\n",
    "\n",
    "### Thresholds\n",
    "Minimum array sizes for parallel processing:\n",
    "- **simple**: Basic reductions (sum, mean, etc.)\n",
    "- **rolling**: Rolling window operations\n",
    "- **robust**: Robust statistics (median, MAD)\n",
    "- **anomaly**: Anomaly detection operations\n",
    "\n",
    "### Threads\n",
    "Number of threads to use for each operation type\n",
    "\n",
    "### NUMA Modes\n",
    "- **0**: Default (OS scheduling)\n",
    "- **1**: Interleaved memory allocation\n",
    "- **2**: Local allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Before and After Calibration\n",
    "\n",
    "Let's measure the impact of calibration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_operations(n=10_000_000, iterations=10):\n",
    "    \"\"\"Benchmark key operations.\"\"\"\n",
    "    data = np.random.randn(n)\n",
    "    \n",
    "    operations = {\n",
    "        'sum': lambda: hpcs.sum(data),\n",
    "        'mean': lambda: hpcs.mean(data),\n",
    "        'std': lambda: hpcs.std(data),\n",
    "        'median': lambda: hpcs.median(data),\n",
    "        'rolling_mean': lambda: hpcs.rolling_mean(data[:100000], 50),\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, func in operations.items():\n",
    "        # Warmup\n",
    "        _ = func()\n",
    "        \n",
    "        # Benchmark\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(iterations):\n",
    "            _ = func()\n",
    "        elapsed = (time.perf_counter() - start) / iterations\n",
    "        results[name] = elapsed * 1000  # Convert to ms\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmark\n",
    "print(\"Benchmarking with current configuration...\\n\")\n",
    "results = benchmark_operations()\n",
    "\n",
    "print(f\"{'Operation':<20} {'Time (ms)':<12} {'Throughput':<15}\")\n",
    "print(\"-\" * 50)\n",
    "for name, time_ms in results.items():\n",
    "    n = 10_000_000 if 'rolling' not in name else 100_000\n",
    "    throughput = n / (time_ms / 1000) / 1e6\n",
    "    print(f\"{name:<20} {time_ms:>8.3f}     {throughput:>8.2f} M elem/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Full Calibration (Optional)\n",
    "\n",
    "Full calibration benchmarks actual performance (30-60 seconds):\n",
    "\n",
    "**Note**: This cell is commented out because it takes time. Uncomment to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run full calibration\n",
    "# print(\"Running full calibration (this may take 30-60 seconds)...\\n\")\n",
    "# start = time.time()\n",
    "# hpcs.calibrate(quick=False)\n",
    "# elapsed = time.time() - start\n",
    "# print(f\"\\nFull calibration completed in {elapsed:.1f} seconds\")\n",
    "# hpcs.save_calibration_config(config_path)\n",
    "# print(f\"Updated configuration saved to: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Scaling Analysis\n",
    "\n",
    "Let's analyze how performance scales with array size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_benchmark(operation, sizes):\n",
    "    \"\"\"Measure operation time across different array sizes.\"\"\"\n",
    "    times = []\n",
    "    \n",
    "    for size in sizes:\n",
    "        data = np.random.randn(size)\n",
    "        \n",
    "        # Warmup\n",
    "        _ = operation(data)\n",
    "        \n",
    "        # Benchmark\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(5):\n",
    "            _ = operation(data)\n",
    "        elapsed = (time.perf_counter() - start) / 5\n",
    "        times.append(elapsed * 1000)  # Convert to ms\n",
    "    \n",
    "    return times\n",
    "\n",
    "# Test different sizes\n",
    "sizes = [1000, 10_000, 100_000, 1_000_000, 10_000_000]\n",
    "\n",
    "# Benchmark sum operation\n",
    "print(\"Benchmarking sum operation across different sizes...\")\n",
    "times_sum = scaling_benchmark(hpcs.sum, sizes)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.loglog(sizes, times_sum, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Array Size', fontsize=12)\n",
    "plt.ylabel('Time (ms)', fontsize=12)\n",
    "plt.title('Sum Operation Performance Scaling', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate throughput\n",
    "print(f\"\\n{'Size':<15} {'Time (ms)':<12} {'Throughput (M elem/s)'}\")\n",
    "print(\"-\" * 50)\n",
    "for size, time_ms in zip(sizes, times_sum):\n",
    "    throughput = size / (time_ms / 1000) / 1e6\n",
    "    print(f\"{size:<15,} {time_ms:>8.3f}     {throughput:>12.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparison: Serial vs Parallel\n",
    "\n",
    "Visualize the benefit of parallelization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This is conceptual - actual serial/parallel comparison\n",
    "# would require modifying the C code to disable parallelization\n",
    "\n",
    "# For demonstration, we'll show the scaling efficiency\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Calculate throughput for each size\n",
    "throughputs = [s / (t / 1000) / 1e6 for s, t in zip(sizes, times_sum)]\n",
    "\n",
    "plt.semilogx(sizes, throughputs, 'o-', linewidth=2, markersize=8, label='HPCSeries (parallel)')\n",
    "plt.axhline(y=throughputs[0], color='r', linestyle='--', alpha=0.7, label='Serial baseline')\n",
    "plt.xlabel('Array Size', fontsize=12)\n",
    "plt.ylabel('Throughput (M elem/s)', fontsize=12)\n",
    "plt.title('Throughput: Parallel Scaling Efficiency', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Peak throughput: {max(throughputs):.2f} M elem/s at size {sizes[throughputs.index(max(throughputs))]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Operation-Specific Tuning\n",
    "\n",
    "Different operations have different optimal configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read current configuration\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Display thresholds\n",
    "thresholds = config['thresholds']\n",
    "threads = config['threads']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot thresholds\n",
    "operations = list(thresholds.keys())\n",
    "threshold_values = list(thresholds.values())\n",
    "ax1.bar(operations, threshold_values, color='steelblue', alpha=0.7)\n",
    "ax1.set_ylabel('Threshold (elements)', fontsize=12)\n",
    "ax1.set_title('Parallelization Thresholds', fontsize=14)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot thread counts\n",
    "thread_values = list(threads.values())\n",
    "ax2.bar(operations, thread_values, color='coral', alpha=0.7)\n",
    "ax2.set_ylabel('Thread Count', fontsize=12)\n",
    "ax2.set_title('Optimal Thread Counts', fontsize=14)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Configuration Summary:\")\n",
    "print(f\"CPU: {config['cpu_id']}\")\n",
    "print(f\"\\nThresholds (min size for parallel):\")\n",
    "for op, thresh in thresholds.items():\n",
    "    print(f\"  {op:<10} {thresh:>10,} elements\")\n",
    "print(f\"\\nThread counts:\")\n",
    "for op, count in threads.items():\n",
    "    print(f\"  {op:<10} {count:>2} threads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Best Practices\n",
    "\n",
    "### When to Calibrate:\n",
    "1. **First installation** - Run calibration once\n",
    "2. **Hardware changes** - After upgrading CPU or RAM\n",
    "3. **Workload changes** - If your typical array sizes change significantly\n",
    "\n",
    "### Calibration Modes:\n",
    "- **Quick calibration** (`quick=True`): Use hardware heuristics, fast (5-10s)\n",
    "- **Full calibration** (`quick=False`): Benchmark actual performance, slower (30-60s) but more accurate\n",
    "\n",
    "### Performance Tips:\n",
    "1. **Use contiguous arrays** - C-contiguous layout is fastest\n",
    "2. **Batch operations** - Process larger arrays when possible\n",
    "3. **Check thresholds** - Ensure your typical array sizes exceed parallelization thresholds\n",
    "4. **Monitor thread count** - Match to your CPU core count for best results\n",
    "\n",
    "### Configuration Persistence:\n",
    "The configuration is saved to `~/.hpcs/config.json` and automatically loaded on import."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. CLI Usage\n",
    "\n",
    "You can also use the command-line interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick calibration via CLI\n",
    "!hpcs calibrate --quick\n",
    "\n",
    "# Show configuration location\n",
    "!hpcs config\n",
    "\n",
    "# Run performance benchmarks\n",
    "!hpcs bench --size 1000000 --iterations 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "HPCSeries Core's auto-tuning calibration system provides:\n",
    "\n",
    "✅ **Automatic optimization** for your hardware  \n",
    "✅ **Operation-specific tuning** (simple, rolling, robust, anomaly)  \n",
    "✅ **Easy API** (`calibrate()`, `save_config()`, `load_config()`)  \n",
    "✅ **CLI support** for automation  \n",
    "✅ **Persistent configuration** across sessions  \n",
    "\n",
    "For most users, running `hpcs.calibrate(quick=True)` once is sufficient for optimal performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
